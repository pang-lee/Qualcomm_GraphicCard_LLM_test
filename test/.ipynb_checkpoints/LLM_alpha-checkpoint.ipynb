{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dae86b7-5008-476b-b378-392461331dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mThank you for using SmartMoneyConcepts! ⭐ Please show your support by giving a star on the GitHub repository: \u001b[4;34mhttps://github.com/joshyattridge/smart-money-concepts\u001b[0m\n",
      "Index: [Timestamp('2024-12-31 17:29:06'), Timestamp('2024-12-31 19:51:07'), Timestamp('2024-12-31 20:00:01')]\n",
      "Columns: ['close_df1', 'volume_df1', 'bid_price_df1', 'ask_price_df1', 'flow_imbalance_df1', 'avg_spread_df1', 'avg_obi_df1', 'sum_price_volume_df1', 'sum_price_sq_volume_df1', 'pv_list_df1', 'monetary_delta_df1', 'volume_delta_df1', 'close_df2', 'volume_df2', 'bid_price_df2', 'ask_price_df2', 'flow_imbalance_df2', 'avg_spread_df2', 'avg_obi_df2', 'sum_price_volume_df2', 'sum_price_sq_volume_df2', 'pv_list_df2', 'monetary_delta_df2', 'volume_delta_df2']\n"
     ]
    }
   ],
   "source": [
    "from smartmoneyconcepts import smc\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from datetime import datetime, timedelta, time\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\n",
    "from scipy.fft import fft, fftfreq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import mplfinance as mpf\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from statsmodels.tsa.stattools import adfuller, coint\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from scipy import stats\n",
    "from arch.unitroot import KPSS\n",
    "from hurst import compute_Hc\n",
    "from statsmodels.tsa.vector_ar.vecm import VECM\n",
    "import seaborn as sns\n",
    "from scipy.integrate import quad\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from scipy.spatial.distance import cityblock, euclidean, cosine\n",
    "from scipy.stats import gaussian_kde, shapiro, probplot, skew, kurtosis, norm, jarque_bera, anderson, normaltest\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "def prepare_data(type_of_data, data_name):\n",
    "    result = type_of_data.split('/')[0]\n",
    "    tmp = pd.read_csv(f'../index_data/{type_of_data}/{data_name}.csv')\n",
    "    if result == 'shioaji':\n",
    "        tmp['ts'] = pd.to_datetime(tmp['ts'])\n",
    "        tmp = tmp.rename(columns=lambda x: x.lower())\n",
    "    else:\n",
    "        tmp['ts'] = pd.to_datetime(tmp['datetime'])\n",
    "        tmp = tmp.rename(columns=lambda x: x.lower())\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "def aggregate_pv_list(pv_lists):\n",
    "    if pv_lists.isna().all():\n",
    "        return np.nan\n",
    "    \n",
    "    combined_volume = defaultdict(float)\n",
    "    for pv in pv_lists.dropna():\n",
    "        if isinstance(pv, list):  # 確保是 list\n",
    "            for item in pv:\n",
    "                if isinstance(item, dict):\n",
    "                    for price, vol in item.items():\n",
    "                        combined_volume[float(price)] += float(vol)  # 轉 float 避免類型問題\n",
    "\n",
    "    # 按價格降序排序並轉為 list of dict\n",
    "    aggregated_pv = [{price: combined_volume[price]} for price in sorted(combined_volume, reverse=True)]\n",
    "    return aggregated_pv\n",
    "\n",
    "def analyze_tick_types(tick_type_series, volume_series):\n",
    "    \"\"\"\n",
    "    分析該秒內的成交類型分布。\n",
    "    外盤成交量為正數，內盤成交量為負數。\n",
    "    \"\"\"\n",
    "    # 將每一筆的 tick_type (1 或 -1) 和 volume 相乘\n",
    "    signed_volumes = [t * v for t, v in zip(tick_type_series, volume_series)]\n",
    "\n",
    "    # 外盤成交量 = 所有正數的總和\n",
    "    outer_vol = sum(vol for vol in signed_volumes if vol > 0)\n",
    "\n",
    "    # 內盤成交量 = 所有負數的總和 (不取絕對值)\n",
    "    inner_vol = sum(vol for vol in signed_volumes if vol < 0)\n",
    "\n",
    "    return outer_vol + inner_vol\n",
    "\n",
    "def calculate_liquidity_factors(bid_list, ask_list, bid_vol_list, ask_vol_list):\n",
    "    try:\n",
    "        # 1. 計算平均買賣價差 (Average Bid-Ask Spread)\n",
    "        spreads = [a - b for a, b in zip(ask_list, bid_list)]\n",
    "        avg_spread = np.mean(spreads) if spreads else np.nan\n",
    "\n",
    "        # 2. 計算委託簿失衡 (Order Book Imbalance, OBI)\n",
    "        obi_list = [\n",
    "            bv / (bv + av) if (bv + av) > 0 else 0.5 \n",
    "            for bv, av in zip(bid_vol_list, ask_vol_list)\n",
    "        ]\n",
    "        # 計算這一秒內所有 tick OBI 的平均值\n",
    "        avg_obi = np.mean(obi_list) if obi_list else np.nan\n",
    "        \n",
    "        return pd.Series({\n",
    "            'avg_spread': avg_spread,\n",
    "            'avg_obi': avg_obi\n",
    "        })\n",
    "        \n",
    "    except (ValueError, TypeError):\n",
    "        return pd.Series({\n",
    "            'avg_spread': np.nan,\n",
    "            'avg_obi': np.nan\n",
    "        })\n",
    "\n",
    "def calculate_price_factors(close_list, volume_list):\n",
    "    \"\"\"\n",
    "    計算秒級價格因子。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sum_price_volume = sum((float(c)) * float(v) for c, v in zip(close_list, volume_list))\n",
    "        sum_price_sq_volume = sum((float(c) ** 2) * float(v) for c, v in zip(close_list, volume_list))\n",
    "        \n",
    "        # 計算 pv_list：按 close 價格聚合成交量\n",
    "        volume_by_close = defaultdict(float)\n",
    "        for close, volume in zip(close_list, volume_list):\n",
    "            volume_by_close[close] += float(volume)\n",
    "        \n",
    "        # 轉為 pv_list 格式並按 close 價格降序排序\n",
    "        pv_list = [{close: volume} for close, volume in sorted(volume_by_close.items(), key=lambda x: x[0], reverse=True)]\n",
    "        \n",
    "        return pd.Series({\n",
    "            'sum_price_volume': sum_price_volume,\n",
    "            'sum_price_sq_volume': sum_price_sq_volume,\n",
    "            'pv_list': pv_list\n",
    "        })\n",
    "    except (ValueError, ZeroDivisionError, TypeError, IndexError):\n",
    "        return pd.Series({\n",
    "            'sum_price_volume': np.nan,\n",
    "            'sum_price_sq_volume': np.nan,\n",
    "            'pv_list': np.nan\n",
    "        })\n",
    "\n",
    "def calculate_capital_factors(bid_list, ask_list, bid_vol_list, ask_vol_list):\n",
    "    \"\"\"\n",
    "    計算秒級資金因子。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 計算名義金額\n",
    "        notional_bids = [p * v for p, v in zip(bid_list, bid_vol_list)]\n",
    "        notional_asks = [p * v for p, v in zip(ask_list, ask_vol_list)]\n",
    "        \n",
    "        # 計算淨資金流向\n",
    "        monetary_delta = sum(notional_bids) - sum(notional_asks)\n",
    "        volume_delta = sum(bid_vol_list) - sum(ask_vol_list)\n",
    "\n",
    "        return pd.Series({\n",
    "            'monetary_delta': monetary_delta,  # CVD 聚合用\n",
    "            'volume_delta': volume_delta,      # 可選，純數量 delta\n",
    "        })\n",
    "    \n",
    "    except (ValueError, ZeroDivisionError, TypeError):\n",
    "        return pd.Series({\n",
    "            'monetary_delta': np.nan,\n",
    "            'volume_delta': np.nan,\n",
    "        })\n",
    "\n",
    "def pre_process(df1, data_type, date_start, date_end):\n",
    "    # 假设你的df已经加载到dataframe\n",
    "    df1['ts'] = pd.to_datetime(df1['ts'])  # 将ts列转换为datetime类型\n",
    "\n",
    "    # 按秒分组，使用 named aggregation\n",
    "    df1 = df1.groupby(df1['ts'].dt.floor('s')).agg(\n",
    "        close=('close', 'last'),\n",
    "        volume=('volume', 'sum'), # 總成交量\n",
    "        close_list=('close', list),\n",
    "        volume_list=('volume', list), # 成交量列表\n",
    "        bid_price=('bid_price', lambda x: tuple(sorted(filter(lambda price: price != 0, x)))),\n",
    "        ask_price=('ask_price', lambda x: tuple(sorted(filter(lambda price: price != 0, x)))),\n",
    "        bid_list=('bid_price', list),\n",
    "        ask_list=('ask_price', list),\n",
    "        bid_vol_list=('bid_volume', list),\n",
    "        ask_vol_list=('ask_volume', list),\n",
    "        tick_type=('tick_type', lambda x: [1 if t == 1 else -1 if t == 2 else 0 for t in x])\n",
    "    ).reset_index()\n",
    "    \n",
    "    # 分析每秒的內外盤成交量\n",
    "    df1['flow_imbalance'] = df1.apply(lambda row: analyze_tick_types(row['tick_type'], row['volume_list']), axis=1)\n",
    "    liquidity_features = df1.apply(lambda row: calculate_liquidity_factors(row['bid_list'], row['ask_list'], row['bid_vol_list'], row['ask_vol_list']), axis=1)\n",
    "    df1 = pd.concat([df1, liquidity_features], axis=1)\n",
    "    \n",
    "    price_features = df1.apply(lambda row: calculate_price_factors(row['close_list'], row['volume_list']), axis=1)\n",
    "    df1 = pd.concat([df1, price_features], axis=1)\n",
    "    \n",
    "    capital_features = df1.apply(lambda row: calculate_capital_factors(row['bid_list'], row['ask_list'], row['bid_vol_list'], row['ask_vol_list']), axis=1)\n",
    "    df1 = pd.concat([df1, capital_features], axis=1)\n",
    "\n",
    "    # 刪除 tick_type 原始資料\n",
    "    df1.drop(columns=['tick_type', 'volume_list', 'bid_list', 'ask_list', 'bid_vol_list', 'ask_vol_list', 'close_list'], inplace=True)\n",
    "\n",
    "    # 创建时间范围从開始到結束天數（或多个天数）\n",
    "    time_range = pd.date_range(date_start, date_end, freq='s')\n",
    "\n",
    "    # 将时间范围转换为DataFrame\n",
    "    full_time_df = pd.DataFrame(time_range, columns=['ts'])\n",
    "    \n",
    "    if data_type == 's_day':\n",
    "        # 通过检查时间是否在9:00:00到13:30:00之间来剔除跨天的数据\n",
    "        valid_time_range = full_time_df['ts'].dt.time.between(pd.to_datetime('09:00:00').time(), pd.to_datetime('13:30:00').time())\n",
    "        valid_time = full_time_df[valid_time_range]\n",
    "        \n",
    "    elif data_type == 'f_day':\n",
    "        # 通过检查时间是否在9:00:00到13:30:00之间来剔除跨天的数据\n",
    "        valid_time_range = full_time_df['ts'].dt.time.between(pd.to_datetime('08:45:00').time(), pd.to_datetime('13:45:00').time())\n",
    "        valid_time = full_time_df[valid_time_range]\n",
    "        \n",
    "    elif data_type == 'f_night':\n",
    "        t1 = pd.to_datetime('08:45:00').time()\n",
    "        t2 = pd.to_datetime('13:45:00').time()\n",
    "        t3 = pd.to_datetime('15:00:00').time()\n",
    "        t4 = pd.to_datetime('05:00:00').time()\n",
    "        \n",
    "        # 提取時間部分並進行向量化比較\n",
    "        time_series = full_time_df['ts'].dt.time\n",
    "        valid_time = full_time_df[\n",
    "            ((time_series >= t1) & (time_series <= t2)) |  # 日盤時間\n",
    "            ((time_series >= t3) | (time_series <= t4))   # 夜盤時間\n",
    "        ]\n",
    "\n",
    "    # 合并df1和df2的结果，确保它们与mer_ori_data按秒对齐, 首先将df1和df2与mer_ori_data合并，使用'left'连接方式，以保留所有有效时间\n",
    "    mer_ori_data = pd.merge(valid_time, df1, on='ts', how='left')\n",
    "\n",
    "    # 设置'ts'为index\n",
    "    mer_ori_data.set_index('ts', inplace=True)\n",
    "    mer_ori_data = mer_ori_data.dropna()\n",
    "    \n",
    "    extra_df = mer_ori_data.resample('1min').agg({\n",
    "        'flow_imbalance': 'sum',\n",
    "        'avg_spread': 'mean',\n",
    "        'avg_obi': 'mean',\n",
    "        'sum_price_volume': 'sum',\n",
    "        'sum_price_sq_volume': 'sum',\n",
    "        'monetary_delta': 'sum',\n",
    "        'volume_delta': 'sum',\n",
    "        'pv_list': aggregate_pv_list\n",
    "    })\n",
    "    \n",
    "    # 為了配合合併分K, 需把時間+1分鐘\n",
    "    extra_df.index = extra_df.index + pd.Timedelta(minutes=1)\n",
    "\n",
    "    # 過濾掉 bid_price 或 ask_price 為空 tuple 的行 (漲停或跌停)\n",
    "    mer_ori_data = mer_ori_data[(mer_ori_data['bid_price'].map(len) > 0) & (mer_ori_data['ask_price'].map(len) > 0)]\n",
    "    \n",
    "    return mer_ori_data.dropna(), extra_df.dropna()\n",
    "\n",
    "def analyze_relationship(series1, series2, significance_level=0.05):\n",
    "    # 計算相關係數\n",
    "    correlation = np.corrcoef(series1, series2)[0, 1]\n",
    "    abs_corr = abs(correlation)\n",
    "    \n",
    "    # 判斷相關性強度\n",
    "    if abs_corr > 0.8:\n",
    "        correlation_strength = \"高度相關\"\n",
    "    elif abs_corr > 0.5:\n",
    "        correlation_strength = \"中度相關\"\n",
    "    elif abs_corr > 0.3:\n",
    "        correlation_strength = \"低度相關\"\n",
    "    else:\n",
    "        correlation_strength = \"幾乎無相關\"\n",
    "    \n",
    "    # 執行共整合檢定\n",
    "    coint_t, p_value, _ = coint(series1, series2)\n",
    "    \n",
    "    # 判斷是否存在共整合關係\n",
    "    if p_value <= significance_level:\n",
    "        cointegration_status = \"存在共整合關係\"\n",
    "    else:\n",
    "        cointegration_status = \"不存在共整合關係\"\n",
    "        \n",
    "    # print(f\"相關係數: {correlation:.4f}，{correlation_strength}\")\n",
    "    # print(f\"共整合檢定的 p 值: {p_value:.4f}，{cointegration_status}\")\n",
    "    \n",
    "    return correlation, correlation_strength, p_value, cointegration_status\n",
    "\n",
    "def convert_ohlcv(df, freq=60):\n",
    "    # 建立 session_type 與 session_start\n",
    "    t1 = datetime.strptime(\"08:45\", \"%H:%M\").time()\n",
    "    t2 = datetime.strptime(\"13:45\", \"%H:%M\").time()\n",
    "    t3 = datetime.strptime(\"05:00\", \"%H:%M\").time()\n",
    "    t4 = datetime.strptime(\"15:00\", \"%H:%M\").time()\n",
    "    \n",
    "    def classify_session(timestamps):\n",
    "        try:\n",
    "            # 1. 使用 .dt.tz 來檢查時區信息\n",
    "            timestamps = timestamps.dt.tz_localize(None) if timestamps.dt.tz is not None else timestamps\n",
    "            \n",
    "            # 2. 使用 .dt.time 和 .dt.date 來提取時間和日期部分\n",
    "            times = timestamps.dt.time\n",
    "            dates = timestamps.dt.date\n",
    "\n",
    "            # 初始化結果\n",
    "            session_type = pd.Series(\"other\", index=timestamps.index)\n",
    "            session_start = pd.Series(pd.NaT, index=timestamps.index)\n",
    "        \n",
    "            # 日盤條件\n",
    "            day_mask = (times >= t1) & (times <= t2)\n",
    "            session_type.loc[day_mask] = \"day\"\n",
    "            session_start.loc[day_mask] = pd.to_datetime(\n",
    "                dates[day_mask].astype(str) + \" \" + t1.strftime(\"%H:%M:%S\")\n",
    "            )\n",
    "    \n",
    "            # 夜盤條件 (當天夜盤)\n",
    "            night_mask1 = (times >= t3) & ~day_mask\n",
    "            session_type.loc[night_mask1] = \"night\"\n",
    "            session_start.loc[night_mask1] = pd.to_datetime(\n",
    "                dates[night_mask1].astype(str) + \" \" + t3.strftime(\"%H:%M:%S\")\n",
    "            )\n",
    "    \n",
    "            # 夜盤條件 (前一天夜盤，時間 <= t4)\n",
    "            night_mask2 = (times <= t4) & ~day_mask & ~night_mask1\n",
    "            session_type.loc[night_mask2] = \"night\"\n",
    "            prev_dates = (timestamps[night_mask2] - timedelta(days=1)).dt.date\n",
    "            session_start.loc[night_mask2] = pd.to_datetime(\n",
    "                prev_dates.astype(str) + \" \" + t3.strftime(\"%H:%M:%S\")\n",
    "            )\n",
    "\n",
    "            return pd.DataFrame({\"session_type\": session_type, \"session_start\": session_start})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"時間轉換出現錯誤: {e}\")\n",
    "            return pd.DataFrame({\"session_type\": None, \"session_start\": None}, index=timestamps.index)\n",
    "    \n",
    "    result = classify_session(df.index.to_series())\n",
    "    df.loc[:, [\"session_type\", \"session_start\"]] = result[[\"session_type\", \"session_start\"]]\n",
    "    df = df[df[\"session_type\"].isin([\"day\", \"night\"])]\n",
    "\n",
    "    # 新增4: 補齊缺失的1分K資料\n",
    "    def fill_missing_minutes(df_session, session_start, session_type):\n",
    "        # 定義交易時段範圍\n",
    "        if session_type == \"day\":\n",
    "            start_time = datetime.combine(session_start.date(), t1)\n",
    "            end_time = datetime.combine(session_start.date(), t2)\n",
    "        else:  # night\n",
    "            start_time = datetime.combine(session_start.date(), t4)\n",
    "            end_time = datetime.combine(session_start.date() + timedelta(days=1), t3)\n",
    "\n",
    "        # 生成完整的1分鐘時間序列\n",
    "        full_time_index = pd.date_range(start=start_time, end=end_time, freq=\"1min\")\n",
    "        existing_times = df_session.index\n",
    "\n",
    "        # 找出缺失的時間點\n",
    "        missing_times = [t for t in full_time_index if t not in existing_times]\n",
    "        \n",
    "        if missing_times:\n",
    "            # 為每個缺失時間點填充資料\n",
    "            missing_data = []\n",
    "            last_valid_row = None\n",
    "            for t in missing_times:\n",
    "                # 找到前一筆有效資料\n",
    "                prev_time = t - timedelta(minutes=1)\n",
    "                if prev_time in df_session.index:\n",
    "                    last_valid_row = df_session.loc[prev_time]\n",
    "                if last_valid_row is not None:\n",
    "                    missing_data.append({\n",
    "                        \"ts\": t,\n",
    "                        \"open\": last_valid_row[\"close\"],\n",
    "                        \"high\": last_valid_row[\"close\"],\n",
    "                        \"low\": last_valid_row[\"close\"],\n",
    "                        \"close\": last_valid_row[\"close\"],\n",
    "                        \"volume\": 0,\n",
    "                        \"amount\": 0,\n",
    "                        \"complete\": True,\n",
    "                        \"session_type\": session_type,\n",
    "                        \"session_start\": session_start\n",
    "                    })\n",
    "\n",
    "            # 將缺失資料合併到原資料\n",
    "            if missing_data:\n",
    "                missing_df = pd.DataFrame(missing_data).set_index(\"ts\")\n",
    "                df_session = pd.concat([df_session, missing_df]).sort_index()\n",
    "\n",
    "        return df_session\n",
    "\n",
    "    # 按 session 分組並補齊缺失資料\n",
    "    df_filled = []\n",
    "    for session_start, session_data in df.groupby(\"session_start\"):\n",
    "        session_type = session_data[\"session_type\"].iloc[0]\n",
    "        session_data = fill_missing_minutes(session_data, session_start, session_type)\n",
    "        df_filled.append(session_data)\n",
    "\n",
    "    if df_filled:\n",
    "        df = pd.concat(df_filled).sort_index()\n",
    "\n",
    "    df.index = df.index - pd.Timedelta(minutes=1)\n",
    "\n",
    "    # 設定 K 棒時間長度\n",
    "    window = timedelta(minutes=freq)\n",
    "\n",
    "    # 分段處理每個 session 的資料\n",
    "    result = []\n",
    "\n",
    "    for session_start, session_data in df.groupby(\"session_start\"):\n",
    "        current_time = session_start\n",
    "        max_time = session_data.index.max()\n",
    "        \n",
    "        session_result = []  # 临时存储当前 session 的 K 棒数据\n",
    "\n",
    "        while current_time < max_time:\n",
    "            next_time = current_time + window\n",
    "            window_data = session_data[(session_data.index >= current_time) & (session_data.index < next_time)]\n",
    "\n",
    "            if not window_data.empty:\n",
    "                o = window_data[\"open\"].iloc[0]\n",
    "                h = window_data[\"high\"].max()\n",
    "                l = window_data[\"low\"].min()\n",
    "                c = window_data[\"close\"].iloc[-1]\n",
    "                v = window_data[\"volume\"].sum()\n",
    "                a = c * v\n",
    "                complete = window_data.index[-1] >= next_time - timedelta(minutes=1)\n",
    "                \n",
    "                # 添加额外的列聚合\n",
    "                flow_imbalance_agg = window_data['flow_imbalance'].sum()\n",
    "                avg_spread_agg = window_data['avg_spread'].mean()\n",
    "                avg_obi_agg = window_data['avg_obi'].mean()\n",
    "                sum_price_volume = window_data['sum_price_volume'].sum()\n",
    "                sum_price_sq_volume = window_data['sum_price_sq_volume'].sum()\n",
    "                monetary_delta_agg = window_data['monetary_delta'].sum()\n",
    "                volume_delta_agg = window_data['volume_delta'].sum()\n",
    "                pv_list_agg = aggregate_pv_list(window_data['pv_list'])\n",
    "\n",
    "                session_result.append({\n",
    "                    \"ts\": current_time,\n",
    "                    \"open\": o,\n",
    "                    \"high\": h,\n",
    "                    \"low\": l,\n",
    "                    \"close\": c,\n",
    "                    \"volume\": v,\n",
    "                    \"amount\": a,\n",
    "                    \"flow_imbalance\": flow_imbalance_agg,\n",
    "                    \"avg_spread\": avg_spread_agg,\n",
    "                    \"avg_obi\": avg_obi_agg,\n",
    "                    \"sum_price_volume\": sum_price_volume,\n",
    "                    \"sum_price_sq_volume\": sum_price_sq_volume,\n",
    "                    \"monetary_delta\": monetary_delta_agg,\n",
    "                    \"volume_delta\": volume_delta_agg,\n",
    "                    \"pv_list\": pv_list_agg,\n",
    "                    \"complete\": complete\n",
    "                })\n",
    "\n",
    "            current_time = next_time\n",
    "            \n",
    "        # 对 session_result 进行累积计算\n",
    "        if session_result:\n",
    "            session_df = pd.DataFrame(session_result)\n",
    "            session_df[\"acc_vol\"] = session_df[\"volume\"].cumsum()  # 在 session 内累积 volume\n",
    "            session_df[\"acc_price_volume\"] = session_df[\"sum_price_volume\"].cumsum()  # 累积 sum_price_volume\n",
    "            session_df[\"acc_price_sq_volume\"] = session_df[\"sum_price_sq_volume\"].cumsum()  # 累积 sum_price_sq_volume\n",
    "            result.extend(session_df.to_dict('records'))\n",
    "\n",
    "    # 建立新的 DataFrame\n",
    "    agg_df = pd.DataFrame(result)\n",
    "    agg_df.set_index(\"ts\", inplace=True, drop=False)\n",
    "    agg_df = agg_df.shift(1).dropna()\n",
    "\n",
    "    return agg_df\n",
    "\n",
    "def combine_daily_k_bars(df):\n",
    "    df = df.copy()\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # 提取日期部分\n",
    "    df['date'] = df.index.date\n",
    "    \n",
    "    # 按日期分組並聚合，並強制 complete=True\n",
    "    combined = df.groupby('date').agg({\n",
    "        'open': 'first',      # 當天第一根 K 棒的開盤價\n",
    "        'high': 'max',        # 當天最高價\n",
    "        'low': 'min',         # 當天最低價\n",
    "        'close': 'last',     # 當天最後一根 K 棒的收盤價\n",
    "        'volume': 'sum',      # 當天總成交量\n",
    "        'complete': lambda x: True  # 強制設為 True\n",
    "    })\n",
    "    \n",
    "    combined['amount'] = combined['close'] * combined['volume']\n",
    "    \n",
    "    # 重置 index 並設置為當天最後一根 K 棒的時間\n",
    "    combined = combined.reset_index()\n",
    "    combined['ts'] = pd.to_datetime(combined['date'])\n",
    "    combined = combined.set_index('ts')\n",
    "    combined = combined.drop('date', axis=1)\n",
    "    \n",
    "    return combined.shift(1).dropna()\n",
    "\n",
    "def process_multiple_datasets(dataset1, dataset2, expensive_commodity, cheap_commodity):\n",
    "    df1_list, df2_list, df3_list, df4_list, extra_list1, extra_list2 = [], [], [], [], [], []\n",
    "\n",
    "    # 处理 tick 数据\n",
    "    for type_of_data, data_name, date_start, date_end, data_type in dataset1:\n",
    "        df = prepare_data(type_of_data, data_name)\n",
    "        df, extra_data = pre_process(df, data_type, date_start, date_end)\n",
    "        if data_name.startswith(expensive_commodity):\n",
    "            df1_list.append(df)\n",
    "            extra_list1.append(extra_data)\n",
    "        elif data_name.startswith(cheap_commodity):\n",
    "            df2_list.append(df)\n",
    "            extra_list2.append(extra_data)\n",
    "\n",
    "    for type_of_data, data_name in dataset2: # 如要使用shioaji的1分K資料\n",
    "        df = prepare_data(type_of_data, data_name)\n",
    "        df = df.set_index('ts')\n",
    "        if data_name.startswith(expensive_commodity + 'k'):\n",
    "            df3_list.append(df)\n",
    "        elif data_name.startswith(cheap_commodity + 'k'):\n",
    "            df4_list.append(df)\n",
    "    \n",
    "    # 合并 df1 和 df2\n",
    "    df1 = pd.concat(df1_list) if df1_list else pd.DataFrame()\n",
    "    df2 = pd.concat(df2_list) if df2_list else pd.DataFrame()\n",
    "    df3 = pd.concat(df3_list) if df3_list else pd.DataFrame()\n",
    "    df4 = pd.concat(df4_list) if df4_list else pd.DataFrame()\n",
    "    extra_data1 = pd.concat(extra_list1) if extra_list1 else pd.DataFrame()\n",
    "    extra_data2 = pd.concat(extra_list2) if extra_list2 else pd.DataFrame()\n",
    "    \n",
    "    # 对 df1 和 df2 进行按秒分组，保留最后一笔（处理重复时间戳）\n",
    "    if not df1.empty:\n",
    "        df1 = df1.groupby(df1.index).last()\n",
    "    if not df2.empty:\n",
    "        df2 = df2.groupby(df2.index).last()\n",
    "\n",
    "    # 对 df3 和 df4 进行按时间戳分组，保留最后一笔（处理重复时间戳）\n",
    "    if not df3.empty:\n",
    "        df3 = df3.groupby(df3.index).last()\n",
    "    if not df4.empty:\n",
    "        df4 = df4.groupby(df4.index).last()\n",
    "\n",
    "    # 合并 df1 和 df2\n",
    "    df = pd.merge(df1, df2, left_index=True, right_index=True, how='inner', suffixes=('_df1', '_df2'))\n",
    "\n",
    "    # 按时间索引排序并删除缺失值\n",
    "    df = df.sort_index().dropna()\n",
    "\n",
    "    # 对 df3 和 df4 按时间索引排序并删除缺失值\n",
    "    if not df3.empty:\n",
    "        df3 = df3.sort_index().dropna()\n",
    "        df3 = pd.merge(\n",
    "            df3,\n",
    "            extra_data1,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"inner\"\n",
    "        )\n",
    "    if not df4.empty:\n",
    "        df4 = df4.sort_index().dropna()\n",
    "        df4 = pd.merge(\n",
    "            df4,\n",
    "            extra_data2,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"inner\"\n",
    "        )\n",
    "\n",
    "    return df, df3, df4\n",
    "\n",
    "dataset1 = [\n",
    "    ('shioaji/2025_0108', 'SOFR1', '2024-12-31 15:00:00', '2025-08-04 13:45:00', 'f_night'),\n",
    "    ('shioaji/2025_0108', 'ZEFR1', '2024-12-31 15:00:00', '2025-08-04 13:45:00', 'f_night')\n",
    "]\n",
    "\n",
    "dataset2 = [\n",
    "    ('shioaji/2025_0108', 'SOFR1k'),\n",
    "    ('shioaji/2025_0108', 'ZEFR1k')\n",
    "]\n",
    "\n",
    "df, df3, df4 = process_multiple_datasets(dataset1, dataset2, 'ZEFR1', 'SOFR1') # 放入貴的, 便宜的, 數值大的在分母, 使得數值可以相除在0~1\n",
    "\n",
    "print(\"Index:\", df.head(3).index.tolist())\n",
    "print(\"Columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07782dcb-9c76-494b-b709-20d5f48e7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_macd(price_series, fast_period=12, slow_period=26, signal_period=9):\n",
    "    # --- 核心計算 ---\n",
    "    # 直接對傳入的 Series 進行計算\n",
    "    # 計算快線 EMA\n",
    "    ema_fast = price_series.ewm(span=fast_period, adjust=False).mean()\n",
    "    \n",
    "    # 計算慢線 EMA\n",
    "    ema_slow = price_series.ewm(span=slow_period, adjust=False).mean()\n",
    "    \n",
    "    # 計算 MACD 線 (DIF)\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    \n",
    "    # 計算訊號線 (DEM)\n",
    "    signal_line = macd_line.ewm(span=signal_period, adjust=False).mean()\n",
    "    \n",
    "    # 計算 MACD 柱狀圖 (Histogram)\n",
    "    histogram = macd_line - signal_line\n",
    "\n",
    "    # --- 找出交叉點 ---\n",
    "    # 比較當前時間點和前一時間點的 MACD 與 Signal 線的相對位置\n",
    "    prev_macd = macd_line.shift(1)\n",
    "    prev_signal = signal_line.shift(1)\n",
    "\n",
    "    # 判斷黃金交叉的條件\n",
    "    is_golden_cross = (macd_line > signal_line) & (prev_macd <= prev_signal)\n",
    "\n",
    "    # 判斷死亡交叉的條件\n",
    "    is_death_cross = (macd_line < signal_line) & (prev_macd >= prev_signal)\n",
    "    \n",
    "    # 使用 np.select 根據條件賦值：黃金交叉為 1，死亡交叉為 -1，其餘為 0\n",
    "    cross = np.select(\n",
    "        [is_golden_cross, is_death_cross], \n",
    "        [1, -1], \n",
    "        default=0\n",
    "    )\n",
    "\n",
    "    # --- 準備最終回傳的結果 ---\n",
    "    # 建立一個新的 DataFrame 來存放所有計算結果\n",
    "    result_df = pd.DataFrame({\n",
    "        'MACD': macd_line,\n",
    "        'Signal': signal_line,\n",
    "        'Histogram': histogram,\n",
    "        'Cross': cross\n",
    "    })\n",
    "    result_df.index = price_series.index\n",
    "    return result_df\n",
    "\n",
    "def align_time_series(df1, df2):\n",
    "    \"\"\"\n",
    "    對齊兩個時間序列 DataFrame 的索引，只保留共有的時間點(K)。\n",
    "    \n",
    "    參數:\n",
    "        df1 (pd.DataFrame): 第一個時間序列 DataFrame\n",
    "        df2 (pd.DataFrame): 第二個時間序列 DataFrame\n",
    "        \n",
    "    返回:\n",
    "        tuple: (對齊後的 df1, 對齊後的 df2) => K\n",
    "    \"\"\"\n",
    "    # 確保索引是 datetime 格式\n",
    "    if not isinstance(df1.index, pd.DatetimeIndex):\n",
    "        df1.index = pd.to_datetime(df1.index)\n",
    "    if not isinstance(df2.index, pd.DatetimeIndex):\n",
    "        df2.index = pd.to_datetime(df2.index)\n",
    "    \n",
    "    # 找出共同的時間點（索引交集）\n",
    "    common_index = df1.index.intersection(df2.index)\n",
    "    \n",
    "    # 檢查是否有共同時間點\n",
    "    if len(common_index) == 0:\n",
    "        raise ValueError(\"No common timestamps found between df1 and df2\")\n",
    "    \n",
    "    # 對齊兩個 DataFrame 的索引\n",
    "    df1_aligned = df1.loc[common_index]\n",
    "    df2_aligned = df2.loc[common_index]\n",
    "    \n",
    "    return df1_aligned, df2_aligned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d570078e-499f-4b31-b64a-01391c4e202f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1636\n"
     ]
    }
   ],
   "source": [
    "K_time = 60\n",
    "CODE = ['ZEFR', 'SOFR']\n",
    "df1_k = convert_ohlcv(df3, K_time)\n",
    "df2_k = convert_ohlcv(df4, K_time)\n",
    "df1_k, df2_k = align_time_series(df1_k, df2_k)\n",
    "df1_k = df1_k.rename(columns={col: col + f'_{CODE[0]}' for col in df1_k.columns})\n",
    "df2_k = df2_k.rename(columns={col: col + f'_{CODE[1]}' for col in df2_k.columns})\n",
    "df_all = df1_k.join(df2_k, how='inner')\n",
    "cols_to_drop = [col for col in df_all.columns if col.startswith('ts_')]\n",
    "\n",
    "df_all['spread'] = np.log1p(df_all[f'close_{CODE[0]}'] / df2_k[f'close_{CODE[1]}'])\n",
    "df_all['flow_spread'] = df_all[f\"flow_imbalance_{CODE[0]}\"] - df_all[f\"flow_imbalance_{CODE[1]}\"]\n",
    "df_all['avg_spread'] = df_all[f\"avg_spread_{CODE[0]}\"] - df_all[f\"avg_spread_{CODE[1]}\"]\n",
    "df_all['avg_obi_spread'] = df_all[f\"avg_obi_{CODE[0]}\"] - df_all[f\"avg_obi_{CODE[1]}\"]\n",
    "df_all['sum_price_volume_spread'] = df_all[f\"sum_price_volume_{CODE[0]}\"] - df_all[f\"sum_price_volume_{CODE[1]}\"]\n",
    "df_all['sum_price_sq_volume_spread'] = df_all[f\"sum_price_sq_volume_{CODE[0]}\"] - df_all[f\"sum_price_sq_volume_{CODE[1]}\"]\n",
    "df_all['monetary_delta_spread'] = df_all[f\"monetary_delta_{CODE[0]}\"] - df_all[f\"monetary_delta_{CODE[1]}\"]\n",
    "df_all['volume_delta_spread'] = df_all[f\"volume_delta_{CODE[0]}\"] - df_all[f\"volume_delta_{CODE[1]}\"]\n",
    "\n",
    "df_all = df_all.drop(columns=cols_to_drop).dropna()\n",
    "print(len(df_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb68ddd1-d6e8-4a17-94b7-664a1986d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_decompose(signal, fs):\n",
    "    N = len(signal)\n",
    "    yf = fft(signal)\n",
    "    xf = fftfreq(N, 1/fs)  # 頻率陣列，長度 N，包含正負頻率\n",
    "    amp = 2.0 / N * np.abs(yf)  # 振幅譜（雙邊轉單邊）\n",
    "    phase = np.angle(yf)  # 相位譜\n",
    "    # 只取正頻率部分 (xf[0] 是 DC，xf[1:N//2] 是正頻率)\n",
    "    return xf[:N//2], amp[:N//2], phase[:N//2], yf\n",
    "\n",
    "def circular_weighted_mean(phases, weights):\n",
    "    if len(phases) == 0:\n",
    "        return np.nan, 0.0  # 空資料回傳 NaN\n",
    "    \n",
    "    weights = np.array(weights, dtype=float)\n",
    "    if np.all(weights == 0):\n",
    "        vec = np.mean(np.exp(1j * phases))\n",
    "    else:\n",
    "        vec = np.sum(weights * np.exp(1j * phases)) / np.sum(weights)\n",
    "        \n",
    "    mean_phase = np.angle(vec)\n",
    "    R = np.abs(vec)\n",
    "    return mean_phase, R\n",
    "\n",
    "def lookback_transform(df_all, **params):\n",
    "    k_lookback = params.get('k_lookback')\n",
    "    results = []\n",
    "    per_freq_summary = []\n",
    "    delta_t = params.get('k_time')\n",
    "    fs = 1 / delta_t\n",
    "    \n",
    "    for i in range(0, len(df_all)):\n",
    "        end_idx = min(i + k_lookback, len(df_all))\n",
    "        df_subset = df_all.iloc[i:end_idx].copy()\n",
    "        \n",
    "        a = calculate_macd(df_subset[f'close_{CODE[0]}'], 8, 12, 5)['Histogram'].dropna()\n",
    "        b = calculate_macd(df_subset[f'close_{CODE[1]}'], 8, 12, 5)['Histogram'].dropna()\n",
    "\n",
    "        xf_a, amp_a, phase_a, yf_a = fft_decompose(a.values, fs)\n",
    "        xf_b, amp_b, phase_b, yf_b = fft_decompose(b.values, fs)\n",
    "        \n",
    "        # 計算每個頻率的相位差 (-pi..pi)\n",
    "        phase_diff = (phase_a - phase_b + np.pi) % (2*np.pi) - np.pi\n",
    "\n",
    "        # 使用 cosine 背離方法計算每個頻率背離強度\n",
    "        divergence_per_freq = (amp_a + amp_b) * (1 - np.cos(phase_diff)) / 2\n",
    "\n",
    "        # scalar summary：總背離強度\n",
    "        total_divergence = np.sum(divergence_per_freq)\n",
    "\n",
    "        # 加權相位差（領先/落後指標）\n",
    "        weights = amp_a + amp_b\n",
    "        weighted_phase_diff, R = circular_weighted_mean(phase_diff, weights)\n",
    "        \n",
    "        # 將加權相位差轉成時間延遲（秒與 K 單位）\n",
    "        if np.sum(weights) == 0:\n",
    "            mean_freq = 0.0\n",
    "        else:\n",
    "            mean_freq = np.sum(weights * xf_a) / np.sum(weights)\n",
    "        if mean_freq == 0:\n",
    "            time_lag_seconds = np.nan\n",
    "            time_lag_in_K = np.nan\n",
    "        else:\n",
    "            time_lag_seconds = weighted_phase_diff / (2 * np.pi * mean_freq)\n",
    "            time_lag_in_K = time_lag_seconds / delta_t\n",
    "        \n",
    "        # 記錄結果於當前結束K \n",
    "        results.append({'ts': df_subset.index[-1], 'total_divergence': total_divergence})\n",
    "        \n",
    "        # 儲存 per-frequency 背離與相位\n",
    "        per_freq_summary.append({\n",
    "            'ts': df_subset.index[-1],\n",
    "            'xf': xf_a,\n",
    "            'amp_a': amp_a,\n",
    "            'amp_b': amp_b,\n",
    "            'phase_a': phase_a,\n",
    "            'phase_b': phase_b,\n",
    "            'phase_diff': phase_diff,\n",
    "            'divergence_per_freq': divergence_per_freq,\n",
    "            'weighted_phase_diff': weighted_phase_diff,\n",
    "            'phase_consistency': R,\n",
    "            'time_lag_seconds': time_lag_seconds,\n",
    "            'time_lag_in_K': time_lag_in_K\n",
    "        })\n",
    "\n",
    "    per_freq_summary_df = pd.DataFrame(per_freq_summary).set_index('ts')\n",
    "    result_df = pd.DataFrame(results).set_index('ts')\n",
    "    return result_df, per_freq_summary_df\n",
    "\n",
    "params = {\n",
    "    'k_lookback': 15,\n",
    "    'k_time': K_time\n",
    "}\n",
    "\n",
    "ftt, per_freq = lookback_transform(df_all, **params)\n",
    "df_all = df_all.join(ftt, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c41bdb9-ffd1-4317-bb22-0ef8767d331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_normal_distribution(similarity, scale=1):\n",
    "    \"\"\"\n",
    "    將相似度值（0~1）映射到類似常態分佈，均值為0，標準差可調。\n",
    "    用於 cosine 相似度，高相似度 (接近 1) 映射到中心 (0)。\n",
    "    \"\"\"\n",
    "    inverted_similarity = 1.0 - similarity\n",
    "    z_score = norm.ppf(inverted_similarity * (1 - 1e-6) + 1e-6)\n",
    "    return z_score * scale\n",
    "\n",
    "def map_manhattan_to_normal(distance, max_distance=3, scale=1):\n",
    "    \"\"\"\n",
    "    將 Manhattan 距離映射到類似常態分佈，均值為0，標準差可調。\n",
    "    distance: 輸入的 Manhattan 距離值\n",
    "    max_distance: 用於正規化的最大距離值\n",
    "    scale: 控制輸出分佈的標準差\n",
    "    \"\"\"\n",
    "    # 正規化距離到 [0, 1]\n",
    "    normalized_distance = np.clip(distance / max_distance, 0.0, 1.0)\n",
    "    # 將正規化距離映射到標準正態分佈的分位數\n",
    "    z_score = norm.ppf(normalized_distance * (1 - 1e-6) + 1e-6)\n",
    "    return z_score * scale\n",
    "\n",
    "def map_euclidean_to_normal(distance, max_distance=3, scale=1):\n",
    "    \"\"\"\n",
    "    將 Euclidean 距離映射到類似常態分佈，均值為0，標準差可調。\n",
    "    distance: 輸入的 Euclidean 距離值\n",
    "    max_distance: 用於正規化的最大距離值\n",
    "    scale: 控制輸出分佈的標準差\n",
    "    \"\"\"\n",
    "    # 正規化距離到 [0, 1]\n",
    "    normalized_distance = np.clip(distance / max_distance, 0.0, 1.0)\n",
    "    # 將正規化距離映射到標準正態分佈的分位數\n",
    "    z_score = norm.ppf(normalized_distance * (1 - 1e-6) + 1e-6)\n",
    "    return z_score * scale\n",
    "\n",
    "def simulate_realtime_vector(df_all, **params):\n",
    "    k_lookback = params.get('k_lookback', 3)  # 預設 3 根 K\n",
    "    code = params.get('code')\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(df_all), k_lookback):\n",
    "        end_idx = min(i + k_lookback, len(df_all))\n",
    "        df1_subset = df_all[[f\"open_{code[0]}\", f\"close_{code[0]}\", f\"low_{code[0]}\", f\"high_{code[0]}\"]].iloc[i:end_idx]\n",
    "        df2_subset = df_all[[f\"open_{code[1]}\", f\"close_{code[1]}\", f\"low_{code[1]}\", f\"high_{code[1]}\"]].iloc[i:end_idx]\n",
    "        df1_subset.columns = ['open', 'close', 'low', 'high']\n",
    "        df2_subset.columns = ['open', 'close', 'low', 'high']\n",
    "        df1_copy = df1_subset.copy()\n",
    "        df2_copy = df2_subset.copy()\n",
    "        \n",
    "        # 如果不足 k_lookback，跳過\n",
    "        if len(df1_copy) < k_lookback or len(df2_copy) < k_lookback:\n",
    "            continue\n",
    "        \n",
    "        # ----------------- A 組計算 -----------------\n",
    "        lowA = df1_copy['low'].min()\n",
    "        highA = df1_copy['high'].max()\n",
    "        totalA = highA - lowA\n",
    "\n",
    "        overlap_lowA = df1_copy['low'].max()\n",
    "        overlap_highA = df1_copy['high'].min()\n",
    "        overlap_lenA = max(0.0, overlap_highA - overlap_lowA)\n",
    "        nonoverlapA = max(0.0, totalA - overlap_lenA) / totalA\n",
    "\n",
    "        # 每根K棒的總長度\n",
    "        rangeA = df1_copy['high'] - df1_copy['low'] + 1e-9   # 避免 0 除錯\n",
    "\n",
    "        # 上下影線（比例）\n",
    "        upper_shadows_A = (df1_copy['high'] - df1_copy[['open', 'close']].max(axis=1)) / rangeA\n",
    "        lower_shadows_A = (df1_copy[['open', 'close']].min(axis=1) - df1_copy['low']) / rangeA\n",
    "\n",
    "        # ----------------- B 組計算 -----------------\n",
    "        lowB = df2_copy['low'].min()\n",
    "        highB = df2_copy['high'].max()\n",
    "        totalB = highB - lowB\n",
    "\n",
    "        overlap_lowB = df2_copy['low'].max()\n",
    "        overlap_highB = df2_copy['high'].min()\n",
    "        overlap_lenB = max(0.0, overlap_highB - overlap_lowB)\n",
    "        nonoverlapB = max(0.0, totalB - overlap_lenB) / totalB\n",
    "\n",
    "        # 每根K棒的總長度\n",
    "        rangeB = df2_copy['high'] - df2_copy['low'] + 1e-9\n",
    "\n",
    "        # 上下影線（比例）\n",
    "        upper_shadows_B = (df2_copy['high'] - df2_copy[['open', 'close']].max(axis=1)) / rangeB\n",
    "        lower_shadows_B = (df2_copy[['open', 'close']].min(axis=1) - df2_copy['low']) / rangeB\n",
    "\n",
    "        # -------- 建立向量 --------\n",
    "        vecA = np.concatenate([[nonoverlapA], upper_shadows_A.values, lower_shadows_A.values])\n",
    "\n",
    "        vecB = np.concatenate([[nonoverlapB], upper_shadows_B.values, lower_shadows_B.values])\n",
    "\n",
    "        # -------- 小擾動避免完全相同 --------\n",
    "        vecA += np.random.normal(0, 1e-6, size=vecA.shape)\n",
    "        vecB += np.random.normal(0, 1e-6, size=vecB.shape)\n",
    "\n",
    "        vecA = np.log1p(vecA)\n",
    "        vecB = np.log1p(vecB)\n",
    "        \n",
    "        # -------- 距離計算 --------\n",
    "        manhattan_dist = map_manhattan_to_normal(cityblock(vecA, vecB))\n",
    "        euclidean_dist = map_euclidean_to_normal(euclidean(vecA, vecB))\n",
    "        cosine_dist = map_to_normal_distribution(1 - cosine(vecA, vecB))\n",
    "        \n",
    "        # -------- 輸出結果 --------\n",
    "        # print(\"A 組向量:\", vecA)\n",
    "        # print(\"B 組向量:\", vecB)\n",
    "        # print(\"Manhattan distance:\", manhattan_dist)\n",
    "        # print(\"Euclidean distance:\", euclidean_dist)\n",
    "        # print(\"Cosine distance:\", cosine_dist, \"\\n\\n\")\n",
    "\n",
    "        results.append({'ts': df1_subset.index[-1], 'similarity': cosine_dist})\n",
    "    \n",
    "    result_df = pd.DataFrame(results).set_index('ts')\n",
    "    return result_df\n",
    "\n",
    "bt_params = {\n",
    "    'k_lookback': 3,\n",
    "    'code': CODE\n",
    "}\n",
    "\n",
    "data = simulate_realtime_vector(df_all, **bt_params)\n",
    "data = data.replace([np.inf, -np.inf], 0).dropna()\n",
    "df_all = df_all.join(data, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24448b53-bce9-4a8a-b5b9-92a48876b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "df_all['flow_spread_trans'] = qt.fit_transform(df_all[['flow_spread']])\n",
    "df_all['avg_spread_trans'] = qt.fit_transform(df_all[['avg_spread']])\n",
    "df_all['avg_obi_spread_trans'] = qt.fit_transform(df_all[['avg_obi_spread']])\n",
    "df_all['sum_price_volume_spread_trans'] = qt.fit_transform(df_all[['sum_price_volume_spread']])\n",
    "df_all['sum_price_sq_volume_spread_trans'] = qt.fit_transform(df_all[['sum_price_sq_volume_spread']])\n",
    "df_all['monetary_delta_spread_trans'] = qt.fit_transform(df_all[['monetary_delta_spread']])\n",
    "df_all['volume_delta_spread_trans'] = qt.fit_transform(df_all[['volume_delta_spread']])\n",
    "df_all['ftt_trans'] = np.arctan(np.sqrt(df_all['total_divergence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ed67189-c6ae-467b-9d92-e7c4354b9835",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SystemMessage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 238\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m預處理完成，所有總結存於\u001b[39m\u001b[38;5;124m\"\u001b[39m, json_file)\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summaries\n\u001b[0;32m--> 238\u001b[0m summaries \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mJSON_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor_split_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 209\u001b[0m, in \u001b[0;36mprocess_windows\u001b[0;34m(df_all, llm, json_file, batch_size, factor_split_size)\u001b[0m\n\u001b[1;32m    190\u001b[0m human_prompt_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124m    請分析以下時間窗口的數據：\u001b[39m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124m    **商品組**: A=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCODE[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, B=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCODE[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124m    請根據以上數據，使用「分析時間段走勢」去預測「預測期時間走勢」。\u001b[39m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# 建 messages\u001b[39;00m\n\u001b[1;32m    208\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 209\u001b[0m     \u001b[43mSystemMessage\u001b[49m(content\u001b[38;5;241m=\u001b[39msystem_prompt),\n\u001b[1;32m    210\u001b[0m     HumanMessage(content\u001b[38;5;241m=\u001b[39mhuman_prompt_content)\n\u001b[1;32m    211\u001b[0m ]\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# 呼叫 LLM\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m正在分析窗口：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_window\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SystemMessage' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json, os, random, re\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "JSON_FILE = f'./history.json'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # 你的本地服務器地址\n",
    "    api_key=\"sk-no-key-required\",  # 如果不需要 API key，用占位符\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",  # 你的 Hugging Face 模型名稱\n",
    "    temperature=0  # 根據需求調整\n",
    ")\n",
    "def clean_think_content(text: str) -> str:\n",
    "    # 將 <think> 到 </think> 中間的內容整段移除\n",
    "    cleaned_text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "    # 去掉前後多餘空白\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def pv_list_profile(pv_list_df, bins=5):\n",
    "    all_prices = []\n",
    "    all_vols = []\n",
    "\n",
    "    # 收集所有價位/量\n",
    "    for col in pv_list_df.columns:\n",
    "        for pv_list in pv_list_df[col]:\n",
    "            if isinstance(pv_list, (list, tuple)):\n",
    "                for d in pv_list:\n",
    "                    for price, vol in d.items():\n",
    "                        all_prices.append(price)\n",
    "                        all_vols.append(vol)\n",
    "\n",
    "    if not all_prices:\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame({'price': all_prices, 'vol': all_vols})\n",
    "\n",
    "    # 分 bin\n",
    "    min_p, max_p = df['price'].min(), df['price'].max()\n",
    "    bins_edges = np.linspace(min_p, max_p, bins+1)\n",
    "    df['bin'] = pd.cut(df['price'], bins=bins_edges, include_lowest=True)\n",
    "    profile = df.groupby('bin', observed=False)['vol'].sum().reset_index()\n",
    "    return profile.to_dict(orient='records')\n",
    "\n",
    "# 將 LLM 分析的 for loop 封裝成一個 function\n",
    "def process_windows(df_all, llm, json_file, batch_size=10, factor_split_size=30):\n",
    "    # 如果 JSON 已存在，刪除它以重新開始\n",
    "    if os.path.exists(json_file):\n",
    "        print(f\"找到舊檔 {json_file}，將其刪除並重新開始分析。\")\n",
    "        os.remove(json_file)\n",
    "\n",
    "    # 無論如何都從一個空的 list 開始\n",
    "    summaries = []\n",
    "    \n",
    "    # 系統提示：更詳細地定義 AI 角色、輸入與輸出\n",
    "    system_prompt = \"\"\"\n",
    "        你是一位頂尖的量化交易分析師，其務是對單一時間點的市場「橫截面快照」進行**法務鑑識等級的歸因分析**。\n",
    "        你的目標是利用分析期(time_window)的因子和spread數據，**直接、明確地解釋**, 預測期(next_window)的未來報酬 (return) 是如何產生的。\n",
    "        交易目標是「發散型配對交易」, 並最終形成一個具體的統計套利關係結果。\n",
    "\n",
    "        【限制】:\n",
    "        1. **僅使用輸入數據**：\n",
    "            - 只能使用 `factor_group` 中提供的因子名稱。\n",
    "            - **禁止引入任何未提供的指標或策略方法**（如 KD, MACD, Bollinger Bands等）。\n",
    "            - 分析語言必須完全基於數據，不得包含策略、理論或技術分析方法描述。\n",
    "        2. **你的所有分析都必須明確引用輸入數據中的「時間範圍」，將你的發現錨定在具體的時間上下文中, 並且在執行分析的時候, 必須使用最左欄的 ts 作為時間軸, 依照ts時間由舊到新, 審視時間上的變化。**\n",
    "        \n",
    "        【數據結構定義】:\n",
    "        你收到的數據分為兩部分：\n",
    "        1.  **分析期 (time_window)**: 時間範圍 (time_window), 當前spread走勢及其包含的所有因子時間序列數據。你的所有分析都基於這個時期的數據變化。\n",
    "        2.  **預測期 (next_window)**: 時間範圍 (next_window), return的數值是未來的時間。這是分析期 (time_window) 行為所導致的「結果」。\n",
    "        \n",
    "        【任務】：\n",
    "        1.**你的任務就是連接 time_window spread走勢和factor_group的「因」vs next_window 的return走勢「果」。**\n",
    "        2. 使用「分析期走勢」去預測「預測期走勢」\n",
    "        \n",
    "      \n",
    "        【請遵守以下規則】：\n",
    "        **第一步事實陳述:趨勢與事件 (Factual Statement: Trends & Events)**\n",
    "        -   **任務**: 在進行任何分析前，首先對輸入的因子進行事實分類和陳述。\n",
    "        -   **輸出格式**:\n",
    "            -   **1a. 連續型因子趨勢**: 對於**連續變化**的因子，按以下格式逐行列出：\n",
    "                -   **因子 [因子名稱]**: 在分析期time_window, (`[時間範圍起始]` 到 `[時間範圍結束]`)，其數值從 `[起始數值]` **變化為** `[結束數值]`。\n",
    "                -   **中間變化包括**： 簡要描述關鍵轉折點或子趨勢，例如 '在[時間點]達到峰值[數值]後下降' 或 '多次正負交叉, 整體盤勢為 `[上升/下降/震盪]`。\n",
    "                -   **盤勢判定範例**: \n",
    "                    - 上升: [10, 11, 10.5, 11.5, 12, 11, 13]\n",
    "                    - 下降: [10, 9.5, 9, 8.5, 8, 8, 7]\n",
    "                    - 震盪: [10, 11, 10, 9.5, 9, 10, 10.5]\n",
    "                    \n",
    "            -   **1b. 事件型因子出現**: 對於**通常為 N/A 的稀有因子**（如 similarity），按以下格式陳述：\n",
    "                -   **因子 [因子名稱]**: 在分析期time_window, 的 `[出現的具體時間點]`，出現`[數值]`。\n",
    "                -   **因子變化**: 紀錄再因子出現後, 和spread之間的變化\n",
    "                    [紀錄範例]\n",
    "                        similarity數據: 2025-01-01 11:00 => 0.7\n",
    "                        本次分析期 2025-01-01 9:00 ~ 2025-01-01 11:00\n",
    "                        在11:00時, 因子similarity出現0.7\n",
    "\n",
    "        **第二步核心歸因：趨勢或事件驅動 (Core Attribution: Trend or Event-Driven)**\n",
    "        -   **任務**: \n",
    "            - 建立一條從「因子變化」或「市場事件」到「未來報酬」的直接因果鏈。\n",
    "            - **如果第一步中識別出「市場事件」，你必須優先使用「事件驅動歸因」範本來構建你的解釋。**\n",
    "            - 把因子的整體走勢, 或者事件出現後因子的變化彼此的關係進行總結。\n",
    "            \n",
    "        -   **輸出格式**: 請參考以下範本，為分別生成歸因陳述：\n",
    "            -   **範本一：因子全為連續型因子**\n",
    "                -   **歸因**:\n",
    "                    -   **預測目標**: 預測期(next_window) 的 `return` 為 `[填入 return 的值]`。\n",
    "                    -   **核心歸因**: \n",
    "                        - 此未來報酬**直接歸因於**分析期(time_window) (`[時間範圍]`) 內 **[因子名稱]** 的趨勢。**具體來說**，該因子從 `[起始時間]` 到 `[結束時間]`，其數值從 `[起始數值]` **變化為** `[結束數值]`，顯示出強烈的 `[看漲/看跌]` 信號。此信號因當時價格處於 `[盤勢情境]` 而被 `[增強/減弱]`。\n",
    "                        - 分析此段時間內因子的盤勢:\n",
    "                        [紀錄範例]\n",
    "                            本次分析期 2025-01-01 9:00 ~ 2025-01-01 12:00\n",
    "                            flow_spread_trans 和 avg_spread_trans盤勢皆為震盪\n",
    "                            avg_obi_spread_trans盤勢為上升 和 flow_spread_trans盤勢為下降, 兩因子走勢整體出現背離\n",
    "                            \n",
    "            -   **範本二：因子由事件因子與連續因子組成**\n",
    "                -   **歸因**:\n",
    "                    -   **預測目標**: 預測期(next_window) 的 `return` 為 `[填入 return 的值]`。\n",
    "                    -   **核心歸因**: 此未來報酬被認為是**對分析期 (time_window) 內一個關鍵市場事件的直接反應**：\n",
    "                        -   **觸發事件**: 在 `[事件發生的具體時間]`，**`[事件因子名稱]`** 出現，出現`[數值]` 信號 (數值為 `[因子值]`)。\n",
    "                        -   **衝擊分析**: 這個高強度事件的出現，其信號意義和其他連續型因子的影響, 在時間範圍內的因子在時間段內同步(異步)盤勢, 直接對未來報酬走勢有影響。\n",
    "                           [範例]\n",
    "                                本次分析期 2025-01-01 9:00 ~ 2025-01-01 12:00\n",
    "                                在similarity出現0.7的時刻(11:00)\n",
    "                                flow_spread_trans的數值由`[前一個時刻(10:00)]`到`[後續時刻(11:00後)]`, 數值由`[數值]`變換為`[數值]`, 因子similarity出現後flow_spread_trans數值上升\n",
    "                                avg_obi_spread_trans的數值由`[前一個時刻(10:00)]`到`[後續時刻(11:00後)]`, 數值由`[數值]`變換為`[數值]`, 因子similarity出現後avg_obi_spread_trans數值下降\n",
    "                                avg_spread_trans的數值由`[前一個時刻(10:00)]`到`[後續時刻(11:00後)]`, 數值由`[數值]`變換為`[數值]`, 因子similarity出現後avg_spread_trans數值先上升(到11:30), 後又下降(到12:00)\n",
    "\n",
    "        **第三步因果關係分析: 統計套利綜合研判 (Statistical Arbitrage Synthesis)**\n",
    "        -   **任務**: 綜合第二步對因子的歸因（無論是趨勢還是事件驅動），形成一個最終的、可操作的「發散型配對交易」建議。\n",
    "        -   **輸出格式**: 你必須輸出以下內容, 包含驅動原因, 信心強度. 貢獻排名：\n",
    "            -   **綜合交易建議**:\n",
    "                -   **核心邏輯**: 當前spread的走勢主要由 **`[趨勢/事件：描述spread主要驅動，例如：因子flow_spread_trans的下降趨勢 + similarity變高]`** 驅動而導致未來的return走勢呈現 `[看漲/看跌]`。\n",
    "                -   **信心強度**: `[高 / 中 / 低]` (如果由「市場事件」驅動，信心強度通常更高)\n",
    "                -   **關鍵觀察點**: 觀察(time_window)的spread走勢和因子, 對於未來return的關係, 使得未來return走勢會上漲或下跌。\n",
    "                -   **對於本次factor_group中有預測貢獻的因子進行排名輸出**\n",
    "                    [貢獻輸出輸出格式舉例如下:]\n",
    "                        本次分析期 2025-01-01 9:00 ~ 2025-01-01 12:00 (spread)\n",
    "                        輸入因子組: ['flow_spread_trans', 'avg_spread_trans', 'avg_obi_spread_trans']\n",
    "                        對於未來走勢預測貢獻排名:\n",
    "                        1. flow_spread_trans\n",
    "                        2. avg_spread_trans\n",
    "                        3. avg_obi_spread_trans\n",
    "\n",
    "        請根據以上框架，對接下來的數據進行分析，確保所有結論都包含明確的時間參考, 需要再輸出時, 將因子變化的時間段和未來報酬的時間段, 也要一併輸出便於後續檢驗。\n",
    "      \"\"\"\n",
    "      \n",
    "    # Step 1: 分離 OHLCV 和 因子\n",
    "    ohlc_cols = ['spread']\n",
    "    factor_cols = [\n",
    "        'flow_spread_trans', 'avg_spread_trans', 'avg_obi_spread_trans', \n",
    "        'sum_price_volume_spread_trans', 'sum_price_sq_volume_spread_trans',\n",
    "        'monetary_delta_spread_trans', 'volume_delta_spread_trans', 'ftt_trans', 'similarity'\n",
    "    ]\n",
    "    \n",
    "    # 提取 df_all 中存在的欄位\n",
    "    available_cols = set(df_all.columns)\n",
    "    k_cols = [col for col in ohlc_cols if col in available_cols]\n",
    "    selected_factor_cols = [col for col in factor_cols if col in available_cols]\n",
    "\n",
    "    # Step 3: 拆分因子成多個小組（隨機打散再切割）\n",
    "    random.shuffle(selected_factor_cols)\n",
    "    factor_groups = [selected_factor_cols[j:j + factor_split_size] for j in range(0, len(selected_factor_cols), factor_split_size)]\n",
    "\n",
    "    for i in range(batch_size, len(df_all), batch_size):\n",
    "        # window_prev: 從當前位置 i 往前取 batch_size 筆數據\n",
    "        window_prev = df_all.iloc[i - batch_size:i]\n",
    "\n",
    "        # window_next: 從當前位置 i 往後取 batch_size 筆數據\n",
    "        window_next = df_all.iloc[i:i + batch_size]\n",
    "        \n",
    "        if window_next.empty:\n",
    "            break  # 如果窗口空了，結束\n",
    "\n",
    "        start_time = window_prev.index[0].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_time = window_prev.index[-1].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        time_window = f\"{start_time} to {end_time}\"\n",
    "        prev_spread_data = window_prev[k_cols]\n",
    "        \n",
    "        start_next_time = window_next.index[0].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_next_time = window_next.index[-1].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        next_window = f\"{start_next_time} to {end_next_time}\"\n",
    "        next_spread_data = window_next[k_cols]\n",
    "\n",
    "        for g_idx, factor_group in enumerate(factor_groups):\n",
    "            partial_factors_str = window_prev[factor_group].to_string(index=True, max_rows=None)\n",
    "\n",
    "            # --- 步驟 3: 將 DataFrame 轉換為結構化的純文字表格 ---\n",
    "            human_prompt_content = f\"\"\"\n",
    "                請分析以下時間窗口的數據：\n",
    "                **商品組**: A={CODE[0]}, B={CODE[1]}\n",
    "\n",
    "                **分析期時間範圍**: {time_window}\n",
    "                    - K棒\n",
    "                    spread={prev_spread_data}\n",
    "                    - 因子 (factor)\n",
    "                    factor_gruop={partial_factors_str}\n",
    "                \n",
    "                **預測期時間範圍**: {next_window}\n",
    "                    - Return (未來報酬參考)\n",
    "                    return={next_spread_data}\n",
    "\n",
    "                請根據以上數據，使用「分析時間段走勢」去預測「預測期時間走勢」。\n",
    "            \"\"\"\n",
    "\n",
    "            # 建立 ChatPromptTemplate\n",
    "            chat_prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", system_prompt),\n",
    "                (\"human\", \"{input_data}\")  # 使用變量占位符\n",
    "            ])\n",
    "            chain = chat_prompt | llm\n",
    "\n",
    "            # 呼叫 LLM\n",
    "            print(f\"正在分析窗口：{time_window}...\")\n",
    "            try:\n",
    "                response = chain.invoke({\"input_data\": human_prompt_content})\n",
    "                summary = clean_think_content(response.content.strip())  # 假設 clean_think_content 已定義\n",
    "            except Exception as e:\n",
    "                summary = f\"錯誤：{e}\"\n",
    "                print(f\"呼叫 LLM 時發生錯誤: {e}\")\n",
    "\n",
    "            # 存成 dict\n",
    "            summaries.append({\n",
    "                \"time_window\": time_window,\n",
    "                \"factor_group\": f\"{g_idx} - {factor_group}\",\n",
    "                \"analysis\": summary\n",
    "            })\n",
    "\n",
    "            # 立即 append 到 JSON (避免中斷遺失)\n",
    "            with open(json_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(summaries, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        print(f\"完成窗口：{time_window}\")\n",
    "\n",
    "    print(\"\\n預處理完成，所有總結存於\", json_file)\n",
    "    return summaries\n",
    "\n",
    "summaries = process_windows(df_all, llm, JSON_FILE, batch_size=10, factor_split_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81c6df-a147-42b0-9622-0d228a7b459d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
