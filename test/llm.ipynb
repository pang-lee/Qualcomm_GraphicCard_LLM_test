{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f213281-8bf3-4daf-90f3-7ab04c752528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mThank you for using SmartMoneyConcepts! ⭐ Please show your support by giving a star on the GitHub repository: \u001b[4;34mhttps://github.com/joshyattridge/smart-money-concepts\u001b[0m\n",
      "Index: [Timestamp('2024-01-02 13:30:00'), Timestamp('2024-01-03 13:30:00'), Timestamp('2024-01-04 13:30:00')]\n",
      "Columns: ['close_2349', 'volume_2349', 'bid_price_2349', 'ask_price_2349', 'flow_imbalance_2349', 'avg_spread_2349', 'avg_obi_2349', 'sum_price_volume_2349', 'sum_price_sq_volume_2349', 'pv_list_2349', 'monetary_delta_2349', 'volume_delta_2349', 'close_3050', 'volume_3050', 'bid_price_3050', 'ask_price_3050', 'flow_imbalance_3050', 'avg_spread_3050', 'avg_obi_3050', 'sum_price_volume_3050', 'sum_price_sq_volume_3050', 'pv_list_3050', 'monetary_delta_3050', 'volume_delta_3050', 'close_8104', 'volume_8104', 'bid_price_8104', 'ask_price_8104', 'flow_imbalance_8104', 'avg_spread_8104', 'avg_obi_8104', 'sum_price_volume_8104', 'sum_price_sq_volume_8104', 'pv_list_8104', 'monetary_delta_8104', 'volume_delta_8104']\n"
     ]
    }
   ],
   "source": [
    "from smartmoneyconcepts import smc\n",
    "# from renko import Renko\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from datetime import datetime, timedelta, time\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\n",
    "from scipy.fft import fft, fftfreq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import mplfinance as mpf\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from statsmodels.tsa.stattools import adfuller, coint\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from collections import defaultdict\n",
    "import math, re, itertools\n",
    "from functools import reduce\n",
    "from scipy import stats\n",
    "from arch.unitroot import KPSS\n",
    "from hurst import compute_Hc\n",
    "from statsmodels.tsa.vector_ar.vecm import VECM\n",
    "import seaborn as sns\n",
    "from scipy.integrate import quad\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from scipy.signal import hilbert\n",
    "from scipy.spatial.distance import cityblock, euclidean, cosine\n",
    "from scipy.stats import gaussian_kde, shapiro, probplot, skew, kurtosis, norm, jarque_bera, anderson, normaltest, entropy, variation\n",
    "from scipy.special import logit, expit\n",
    "# from finmlkit.bar.base import TradesData\n",
    "# from finmlkit.bar.kit import VolumeBarKit, TickBarKit, DollarBarKit, TimeBarKit, CUSUMBarKit\n",
    "# from finmlkit.feature.kit import Feature, FeatureKit, Compose\n",
    "# from finmlkit.feature import transforms as ts\n",
    "# from finmlkit.feature.base import SISOTransform, SIMOTransform, MISOTransform, BaseTransform, MIMOTransform\n",
    "# from finmlkit.feature.core.volatility import parkinson_range\n",
    "# from finmlkit.feature.core.momentum import roc\n",
    "import talib\n",
    "\n",
    "def prepare_data(type_of_data, data_name):\n",
    "    result = type_of_data.split('/')[0]\n",
    "    tmp = pd.read_csv(f'../index_data/{type_of_data}/{data_name}.csv')\n",
    "    if result == 'shioaji':\n",
    "        tmp['ts'] = pd.to_datetime(tmp['ts'])\n",
    "        tmp = tmp.rename(columns=lambda x: x.lower())\n",
    "    else:\n",
    "        tmp['ts'] = pd.to_datetime(tmp['datetime'])\n",
    "        tmp = tmp.rename(columns=lambda x: x.lower())\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "def aggregate_pv_list(pv_lists):\n",
    "    if pv_lists.isna().all():\n",
    "        return np.nan\n",
    "    \n",
    "    combined_volume = defaultdict(float)\n",
    "    for pv in pv_lists.dropna():\n",
    "        if isinstance(pv, list):  # 確保是 list\n",
    "            for item in pv:\n",
    "                if isinstance(item, dict):\n",
    "                    for price, vol in item.items():\n",
    "                        combined_volume[float(price)] += float(vol)  # 轉 float 避免類型問題\n",
    "\n",
    "    # 按價格降序排序並轉為 list of dict\n",
    "    aggregated_pv = [{price: combined_volume[price]} for price in sorted(combined_volume, reverse=True)]\n",
    "    return aggregated_pv\n",
    "\n",
    "def filter_market_data(bid_list, ask_list, bid_vol_list, ask_vol_list, check_abnormal=False):\n",
    "    valid_data = []\n",
    "    abnormal_data = []\n",
    "\n",
    "    # 使用 zip 將四個列表的對應元素打包成元組\n",
    "    # zip 會自動處理列表長度不一的情況，以最短的為準\n",
    "    for bid, ask, bid_vol, ask_vol in zip(bid_list, ask_list, bid_vol_list, ask_vol_list):\n",
    "        \n",
    "        # 判斷是否為異常數據\n",
    "        # 只要滿足以下任一條件，即為異常\n",
    "        is_abnormal = (\n",
    "            ask < bid or          # 條件1: 賣價低於買價\n",
    "            bid <= 0 or           # 條件2: 買價無效\n",
    "            ask <= 0 or           # 條件2: 賣價無效\n",
    "            bid_vol <= 0 or       # 條件2: 買量無效\n",
    "            ask_vol <= 0          # 條件2: 賣量無效\n",
    "        )\n",
    "\n",
    "        if is_abnormal:\n",
    "            abnormal_data.append((bid, ask, bid_vol, ask_vol))\n",
    "        else:\n",
    "            valid_data.append((bid, ask, bid_vol, ask_vol))\n",
    "\n",
    "    # 如果發現了異常數據，則打印出來\n",
    "    if abnormal_data and check_abnormal is True:\n",
    "        print(f\"发现 {len(abnormal_data)} 笔异常数据，已将其过滤：\")\n",
    "        for i, (b, a, bv, av) in enumerate(abnormal_data):\n",
    "            print(f\"  第{i+1}笔异常: 买价={b}, 卖价={a}, 买量={bv}, 卖量={av}\")\n",
    "\n",
    "    # 如果沒有任何有效的數據，返回四個空列表\n",
    "    if not valid_data:\n",
    "        return [], [], [], []\n",
    "\n",
    "    # 將過濾後的有效數據解包，並返回四個新的列表\n",
    "    valid_bids, valid_asks, valid_bid_vols, valid_ask_vols = zip(*valid_data)\n",
    "    \n",
    "    # zip返回的是元組(tuple)，我們將其轉為列表(list)再返回\n",
    "    return list(valid_bids), list(valid_asks), list(valid_bid_vols), list(valid_ask_vols)\n",
    "\n",
    "def analyze_tick_types(tick_type_series, volume_series):\n",
    "    \"\"\"\n",
    "    分析該秒內的成交類型分布。\n",
    "    外盤成交量為正數，內盤成交量為負數。\n",
    "    \"\"\"\n",
    "    # 將每一筆的 tick_type (1 或 -1) 和 volume 相乘\n",
    "    signed_volumes = [t * v for t, v in zip(tick_type_series, volume_series)]\n",
    "\n",
    "    # 外盤成交量 = 所有正數的總和\n",
    "    outer_vol = sum(vol for vol in signed_volumes if vol > 0)\n",
    "\n",
    "    # 內盤成交量 = 所有負數的總和 (不取絕對值)\n",
    "    inner_vol = sum(vol for vol in signed_volumes if vol < 0)\n",
    "\n",
    "    return outer_vol + inner_vol\n",
    "\n",
    "def calculate_liquidity_factors(bid_list, ask_list, bid_vol_list, ask_vol_list, vol_list):\n",
    "    clean_bids, clean_asks, clean_bid_vols, clean_ask_vols = filter_market_data(\n",
    "        bid_list, ask_list, bid_vol_list, ask_vol_list\n",
    "    )\n",
    "    \n",
    "    if not clean_bids:  # 檢查任一列表即可，因為它們要麼全有數據，要麼全空\n",
    "        return pd.Series({\n",
    "            'avg_spread': 0.0,\n",
    "            'avg_obi': 0.5  # 或者 0.5，取決於您對無數據時的 OBI 定義\n",
    "        })\n",
    "    \n",
    "    try:\n",
    "        # 初始化\n",
    "        Wtd_Spread_Num = 0.0\n",
    "        Wtd_OBI_Num = 0.0\n",
    "        N = len(clean_bids)\n",
    "        \n",
    "        # 處理每一筆 tick\n",
    "        for i in range(N):\n",
    "            # 1. 計算即時價差 (Spread_i)\n",
    "            spread_i = clean_asks[i] - clean_bids[i]\n",
    "\n",
    "            # 2. 計算即時 OBI (OBI_i)\n",
    "            # OBI = Vb / (Vb + Va)，注意避免分母為零\n",
    "            bid_vol_i = clean_bid_vols[i]\n",
    "            ask_vol_i = clean_ask_vols[i]\n",
    "\n",
    "            den_i = bid_vol_i + ask_vol_i\n",
    "            if den_i > 0:\n",
    "                obi_i = bid_vol_i / den_i\n",
    "            else:\n",
    "                obi_i = 0.5 # 如果沒有掛單量，OBI 設為中性 0.5\n",
    "\n",
    "            # 3. 獲取成交量作為權重 (Volume_Traded_i)\n",
    "            vol_traded_i = vol_list[i]\n",
    "\n",
    "            # 4. 累積加權分子總和\n",
    "            Wtd_Spread_Num += spread_i * vol_traded_i\n",
    "            Wtd_OBI_Num += obi_i * vol_traded_i\n",
    "            \n",
    "        return pd.Series({\n",
    "            'avg_spread': Wtd_Spread_Num,\n",
    "            'avg_obi': Wtd_OBI_Num\n",
    "        })\n",
    "        \n",
    "    except (ValueError, TypeError):\n",
    "        return pd.Series({\n",
    "            'avg_spread': np.nan,\n",
    "            'avg_obi': np.nan\n",
    "        })\n",
    "\n",
    "def calculate_price_factors(close_list, volume_list):\n",
    "    \"\"\"\n",
    "    計算秒級價格因子。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sum_price_volume = sum((float(c)) * float(v) for c, v in zip(close_list, volume_list))\n",
    "        sum_price_sq_volume = sum((float(c) ** 2) * float(v) for c, v in zip(close_list, volume_list))\n",
    "        \n",
    "        # 計算 pv_list：按 close 價格聚合成交量\n",
    "        volume_by_close = defaultdict(float)\n",
    "        for close, volume in zip(close_list, volume_list):\n",
    "            volume_by_close[close] += float(volume)\n",
    "        \n",
    "        # 轉為 pv_list 格式並按 close 價格降序排序\n",
    "        pv_list = [{close: volume} for close, volume in sorted(volume_by_close.items(), key=lambda x: x[0], reverse=True)]\n",
    "        \n",
    "        return pd.Series({\n",
    "            'sum_price_volume': sum_price_volume,\n",
    "            'sum_price_sq_volume': sum_price_sq_volume,\n",
    "            'pv_list': pv_list\n",
    "        })\n",
    "    except (ValueError, ZeroDivisionError, TypeError, IndexError):\n",
    "        return pd.Series({\n",
    "            'sum_price_volume': np.nan,\n",
    "            'sum_price_sq_volume': np.nan,\n",
    "            'pv_list': np.nan\n",
    "        })\n",
    "\n",
    "def calculate_capital_factors(bid_list, ask_list, bid_vol_list, ask_vol_list):\n",
    "    \"\"\"\n",
    "    計算秒級資金因子。\n",
    "    \"\"\"\n",
    "    clean_bids, clean_asks, clean_bid_vols, clean_ask_vols = filter_market_data(\n",
    "        bid_list, ask_list, bid_vol_list, ask_vol_list\n",
    "    )\n",
    "    \n",
    "    if not clean_bids:\n",
    "        return pd.Series({\n",
    "            'monetary_delta': 0.0,\n",
    "            'volume_delta': 0.0,\n",
    "        })\n",
    "    \n",
    "    try:\n",
    "        # 計算名義金額\n",
    "        notional_bids = [p * v for p, v in zip(clean_bids, clean_bid_vols)]\n",
    "        notional_asks = [p * v for p, v in zip(clean_asks, clean_ask_vols)]\n",
    "        \n",
    "        # 計算淨資金流向\n",
    "        monetary_delta = sum(notional_bids) - sum(notional_asks)\n",
    "        volume_delta = sum(clean_bid_vols) - sum(clean_ask_vols)\n",
    "        \n",
    "        return pd.Series({\n",
    "            'monetary_delta': monetary_delta,  # CVD 聚合用\n",
    "            'volume_delta': volume_delta,      # 可選，純數量 delta\n",
    "        })\n",
    "    \n",
    "    except (ValueError, ZeroDivisionError, TypeError):\n",
    "        return pd.Series({\n",
    "            'monetary_delta': np.nan,\n",
    "            'volume_delta': np.nan,\n",
    "        })\n",
    "\n",
    "def build_finmlkit_trade(df):\n",
    "    origin_df = df.copy()\n",
    "    origin_df['tick_type'] = origin_df['tick_type'].map({1: -1, 2: 1}).fillna(0).astype(np.int8)\n",
    "    origin_df.index = pd.to_datetime(origin_df['ts'])\n",
    "    ts_ns = origin_df.index.view(np.int64)\n",
    "    \n",
    "    trades_data = TradesData(\n",
    "        ts=ts_ns,\n",
    "        px=origin_df['close'].values,\n",
    "        qty=origin_df['volume'].values,\n",
    "        dt_index=origin_df.index,\n",
    "        side=origin_df['tick_type'].values,\n",
    "        timestamp_unit='ns'   # 明確告訴它單位是納秒\n",
    "    )\n",
    "    \n",
    "    return trades_data\n",
    "\n",
    "def pre_process_by_finmlkit_volbar(df, volume_threshold):\n",
    "    trade = build_finmlkit_trade(df)\n",
    "    volume_bar_kit = VolumeBarKit(\n",
    "        trades=trade,\n",
    "        volume_ths=volume_threshold\n",
    "    )\n",
    "\n",
    "    return volume_bar_kit\n",
    "\n",
    "def pre_process_by_finmlkit_tickbar(df, n):\n",
    "    trade = build_finmlkit_trade(df)\n",
    "    tick_bar_kit = TickBarKit(\n",
    "        trades=trade,\n",
    "        tick_count_thrs=n\n",
    "    )\n",
    "\n",
    "    return tick_bar_kit\n",
    "\n",
    "def pre_process_by_finmlkit_dollarbar(df, dollar_threshold):\n",
    "    trade = build_finmlkit_trade(df)\n",
    "    dollar_bar_kit = DollarBarKit(\n",
    "        trades=trade,\n",
    "        dollar_thrs=dollar_threshold # ✅ 每 1,000,000 (1_000_000) 元成交金額形成一根 bar（可調整）\n",
    "    )\n",
    "\n",
    "    return dollar_bar_kit\n",
    "\n",
    "def pre_process_by_finmlkit_timebar(df, time):\n",
    "    trade = build_finmlkit_trade(df)\n",
    "    time_bar_kit = TimeBarKit(\n",
    "        trades=trade,\n",
    "        period=pd.Timedelta(time) # ✅ 每 1 分鐘形成一根 bar，可改'1min', '5s', '10s', '1h' 等\n",
    "    )\n",
    "    \n",
    "    return time_bar_kit\n",
    "\n",
    "def pre_process_by_finmlkit_sumbar(df, window, min_sigma, sigma_mult):\n",
    "    trade = build_finmlkit_trade(df)\n",
    "    \n",
    "    # 使用 log-returns 與 rolling std，window 可調（例如 200 ticks）\n",
    "    prices = df['close'].astype(float)\n",
    "    logret = np.log(prices).diff().fillna(0.0)\n",
    "    \n",
    "    # rolling std on returns (align with ticks)\n",
    "    rolling_std = logret.rolling(window=window, min_periods=1, center=False).std().to_numpy(dtype=np.float64)\n",
    "    \n",
    "    # 如果 rolling_std 太小或為 0，會在 CUSUM 裡用 sigma_floor 修正\n",
    "    sigma_vector = rolling_std \n",
    "    \n",
    "    cusum_bar_kit = CUSUMBarKit(\n",
    "        trades=trade,\n",
    "        sigma=sigma_vector, # 1d np.ndarray 或常數\n",
    "        sigma_floor=min_sigma,   # 最小 sigma（可視價格單位或 log-return 單位調）\n",
    "        sigma_mult=sigma_mult      # lambda_th = sigma_mult * max(sigma, sigma_floor)\n",
    "    )\n",
    "    \n",
    "    return cusum_bar_kit\n",
    "\n",
    "def concate_finmlkit(list_of_obj_dict, name):\n",
    "    ohlcv_dict = {}\n",
    "    direction_dict = {}\n",
    "    size_dict = {}\n",
    "\n",
    "    for code, obj_list in list_of_obj_dict.items():\n",
    "        ohlcv_list, direction_list, size_list = [], [], []\n",
    "        for item in obj_list:\n",
    "            ohlcv_list.append(item.build_ohlcv())\n",
    "            direction_list.append(item.build_directional_features())\n",
    "            n_bars = len(item._close_indices) - 1\n",
    "            theta_array = np.full(n_bars, 10, dtype=np.float64)\n",
    "            size_list.append(item.build_trade_size_features(theta=theta_array))\n",
    "        \n",
    "        # 合併各項資料\n",
    "        ohlcv_df = pd.concat(ohlcv_list) if ohlcv_list else pd.DataFrame()\n",
    "        direction_df = pd.concat(direction_list) if direction_list else pd.DataFrame()\n",
    "        size_df = pd.concat(size_list) if size_list else pd.DataFrame()\n",
    "\n",
    "        # 對時間索引整合\n",
    "        if not ohlcv_df.empty:\n",
    "            ohlcv_df = ohlcv_df.groupby(ohlcv_df.index).last().sort_index()\n",
    "        if not direction_df.empty:\n",
    "            direction_df = direction_df.groupby(direction_df.index).last().sort_index()\n",
    "        if not size_df.empty:\n",
    "            size_df = size_df.groupby(size_df.index).last().sort_index()\n",
    "\n",
    "        # 存進字典\n",
    "        ohlcv_dict[code] = ohlcv_df.add_suffix(f\"_{name}_{code}\")\n",
    "        direction_dict[code] = direction_df.add_suffix(f\"_{name}_{code}\")\n",
    "        size_dict[code] = size_df.add_suffix(f\"_{name}_{code}\")\n",
    "\n",
    "    # === 對齊多商品資料 === #\n",
    "    def merge_all(dfs_dict):\n",
    "        non_empty = [df for df in dfs_dict.values() if not df.empty]\n",
    "        if not non_empty:\n",
    "            return pd.DataFrame()\n",
    "        merged = pd.concat(non_empty, axis=1)\n",
    "        merged = merged.sort_index().ffill().dropna()\n",
    "        return merged\n",
    "\n",
    "    df_kbar = merge_all(ohlcv_dict)\n",
    "    df_direction = merge_all(direction_dict)\n",
    "    df_size = merge_all(size_dict)\n",
    "\n",
    "    return df_kbar, df_direction, df_size\n",
    "\n",
    "def pre_process_by_range(df, range_size=0.005):\n",
    "    df = df.copy()\n",
    "    df['ts'] = pd.to_datetime(df['ts'])\n",
    "    df = df.sort_values('ts').reset_index(drop=True)\n",
    "    df.rename(columns={'close': 'tick_close'}, inplace=True)\n",
    "    \n",
    "    # 計算買賣量\n",
    "    df['buy_volume'] = np.where(df['tick_type'] == 1, df['volume'], 0)\n",
    "    df['sell_volume'] = np.where(df['tick_type'] == 2, df['volume'], 0)\n",
    "\n",
    "    # 初始化 bar_id\n",
    "    prices = df['tick_close'].values\n",
    "    n = len(df)\n",
    "    bar_ids = np.zeros(n, dtype=int)\n",
    "    current_bar_id = 0\n",
    "    bar_open = prices[0]\n",
    "    upper_bound = bar_open * (1 + range_size)\n",
    "    lower_bound = bar_open * (1 - range_size)\n",
    "\n",
    "    # 向量化生成 bar_id\n",
    "    for i in range(1, n):\n",
    "        price = prices[i]\n",
    "        if price >= upper_bound or price <= lower_bound:\n",
    "            current_bar_id += 1\n",
    "            bar_open = price\n",
    "            upper_bound = bar_open * (1 + range_size)\n",
    "            lower_bound = bar_open * (1 - range_size)\n",
    "        bar_ids[i] = current_bar_id\n",
    "\n",
    "    df['bar_id'] = bar_ids\n",
    "\n",
    "    # 按 bar_id 聚合，生成 K 線數據\n",
    "    bar_stats = df.groupby('bar_id').agg(\n",
    "        ts_range=('ts', 'first'),                    # K線開始時間\n",
    "        open_range=('tick_close', 'first'),          # 開盤\n",
    "        high_range=('tick_close', 'max'),            # 最高\n",
    "        low_range=('tick_close', 'min'),             # 最低\n",
    "        close_range=('tick_close', 'last'),          # 收盤\n",
    "        volume_range=('volume', 'sum'),              # 總成交量\n",
    "        buy_volume_range=('buy_volume', 'sum'),      # 主動買量\n",
    "        sell_volume_range=('sell_volume', 'sum')     # 主動賣量\n",
    "    ).reset_index()\n",
    "    \n",
    "    # 計算 order_imbalance\n",
    "    total_vol = bar_stats['buy_volume_range'] + bar_stats['sell_volume_range']\n",
    "    bar_stats['order_imbalance_range'] = np.where(\n",
    "        total_vol > 0,\n",
    "        (bar_stats['buy_volume_range'] - bar_stats['sell_volume_range']) / total_vol,\n",
    "        0\n",
    "    )\n",
    "\n",
    "    # 設定 ts_range 為 index\n",
    "    bar_stats = bar_stats.set_index('ts_range')\n",
    "\n",
    "    # 最終欄位順序（保留 bar_id）\n",
    "    final_columns = [\n",
    "        'bar_id',\n",
    "        'open_range', 'high_range', 'low_range', 'close_range',\n",
    "        'volume_range', 'buy_volume_range', 'sell_volume_range',\n",
    "        'order_imbalance_range'\n",
    "    ]\n",
    "    \n",
    "    return bar_stats[final_columns]\n",
    "\n",
    "def pre_process_by_renko(df, brick_size=0.5, renko_type='normal'):\n",
    "    df = df.set_index('ts')\n",
    "    r = Renko(df, brick_size=brick_size)\n",
    "    rdf = r.renko_df(renko_type)\n",
    "    rdf = rdf.rename(columns={\n",
    "        'open': 'open_renko',\n",
    "        'high': 'high_renko',\n",
    "        'low': 'low_renko',\n",
    "        'close': 'close_renko',\n",
    "        'volume': 'volume_renko'\n",
    "    })\n",
    "    return rdf\n",
    "\n",
    "def pre_process_by_second(df1, data_type, date_start, date_end):\n",
    "    # 假设你的df已经加载到dataframe\n",
    "    df1['ts'] = pd.to_datetime(df1['ts'])  # 将ts列转换为datetime类型\n",
    "\n",
    "    # 按秒分组，使用 named aggregation\n",
    "    df1 = df1.groupby(df1['ts'].dt.floor('s')).agg(\n",
    "        close=('close', 'last'),\n",
    "        volume=('volume', 'sum'), # 總成交量\n",
    "        close_list=('close', list),\n",
    "        volume_list=('volume', list), # 成交量列表\n",
    "        bid_price=('bid_price', lambda x: tuple(sorted(filter(lambda price: price != 0, x)))),\n",
    "        ask_price=('ask_price', lambda x: tuple(sorted(filter(lambda price: price != 0, x)))),\n",
    "        bid_list=('bid_price', list),\n",
    "        ask_list=('ask_price', list),\n",
    "        bid_vol_list=('bid_volume', list),\n",
    "        ask_vol_list=('ask_volume', list),\n",
    "        tick_type=('tick_type', lambda x: [1 if t == 1 else -1 if t == 2 else 0 for t in x])\n",
    "    ).reset_index()\n",
    "    \n",
    "    # 分析每秒的內外盤成交量\n",
    "    df1['flow_imbalance'] = df1.apply(lambda row: analyze_tick_types(row['tick_type'], row['volume_list']), axis=1)\n",
    "    liquidity_features = df1.apply(lambda row: calculate_liquidity_factors(row['bid_list'], row['ask_list'], row['bid_vol_list'], row['ask_vol_list'], row['volume_list']), axis=1)\n",
    "    df1 = pd.concat([df1, liquidity_features], axis=1)\n",
    "    \n",
    "    price_features = df1.apply(lambda row: calculate_price_factors(row['close_list'], row['volume_list']), axis=1)\n",
    "    df1 = pd.concat([df1, price_features], axis=1)\n",
    "    \n",
    "    capital_features = df1.apply(lambda row: calculate_capital_factors(row['bid_list'], row['ask_list'], row['bid_vol_list'], row['ask_vol_list']), axis=1)\n",
    "    df1 = pd.concat([df1, capital_features], axis=1)\n",
    "\n",
    "    df1.drop(columns=['tick_type', 'volume_list', 'bid_list', 'ask_list', 'bid_vol_list', 'ask_vol_list', 'close_list'], inplace=True)\n",
    "\n",
    "    # 创建时间范围从開始到結束天數（或多个天数）\n",
    "    time_range = pd.date_range(date_start, date_end, freq='s')\n",
    "\n",
    "    # 将时间范围转换为DataFrame\n",
    "    full_time_df = pd.DataFrame(time_range, columns=['ts'])\n",
    "    \n",
    "    if data_type == 's_day':\n",
    "        # 通过检查时间是否在9:00:00到13:30:00之间来剔除跨天的数据\n",
    "        valid_time_range = full_time_df['ts'].dt.time.between(pd.to_datetime('09:00:00').time(), pd.to_datetime('13:30:00').time())\n",
    "        valid_time = full_time_df[valid_time_range]\n",
    "        \n",
    "    elif data_type == 'f_day':\n",
    "        # 通过检查时间是否在9:00:00到13:30:00之间来剔除跨天的数据\n",
    "        valid_time_range = full_time_df['ts'].dt.time.between(pd.to_datetime('08:45:00').time(), pd.to_datetime('13:45:00').time())\n",
    "        valid_time = full_time_df[valid_time_range]\n",
    "        \n",
    "    elif data_type == 'f_night':\n",
    "        t1 = pd.to_datetime('08:45:00').time()\n",
    "        t2 = pd.to_datetime('13:45:00').time()\n",
    "        t3 = pd.to_datetime('15:00:00').time()\n",
    "        t4 = pd.to_datetime('05:00:00').time()\n",
    "        \n",
    "        # 提取時間部分並進行向量化比較\n",
    "        time_series = full_time_df['ts'].dt.time\n",
    "        valid_time = full_time_df[\n",
    "            ((time_series >= t1) & (time_series <= t2)) |  # 日盤時間\n",
    "            ((time_series >= t3) | (time_series <= t4))   # 夜盤時間\n",
    "        ]\n",
    "\n",
    "    # 合并df1和df2的结果，确保它们与mer_ori_data按秒对齐, 首先将df1和df2与mer_ori_data合并，使用'left'连接方式，以保留所有有效时间\n",
    "    mer_ori_data = pd.merge(valid_time, df1, on='ts', how='left')\n",
    "\n",
    "    # 设置'ts'为index\n",
    "    mer_ori_data.set_index('ts', inplace=True)\n",
    "    mer_ori_data = mer_ori_data.dropna()\n",
    "    \n",
    "    extra_df = mer_ori_data.resample('1min').agg({\n",
    "        'flow_imbalance': 'sum',\n",
    "        'avg_spread': 'sum',\n",
    "        'avg_obi': 'sum',\n",
    "        'sum_price_volume': 'sum',\n",
    "        'sum_price_sq_volume': 'sum',\n",
    "        'monetary_delta': 'sum',\n",
    "        'volume_delta': 'sum',\n",
    "        'pv_list': aggregate_pv_list\n",
    "    })\n",
    "    \n",
    "    # 為了配合合併分K, 需把時間+1分鐘\n",
    "    extra_df.index = extra_df.index + pd.Timedelta(minutes=1)\n",
    "    \n",
    "    # 過濾掉 bid_price 或 ask_price 為空 tuple 的行 (漲停或跌停)\n",
    "    mer_ori_data = mer_ori_data[(mer_ori_data['bid_price'].map(len) > 0) & (mer_ori_data['ask_price'].map(len) > 0)]\n",
    "    \n",
    "    return mer_ori_data.dropna(), extra_df.dropna()\n",
    "\n",
    "def convert_ohlcv(df, freq=60):\n",
    "    # 建立 session_type 與 session_start\n",
    "    t1 = datetime.strptime(\"08:45\", \"%H:%M\").time()\n",
    "    t2 = datetime.strptime(\"13:45\", \"%H:%M\").time()\n",
    "    t3 = datetime.strptime(\"05:00\", \"%H:%M\").time()\n",
    "    t4 = datetime.strptime(\"15:00\", \"%H:%M\").time()\n",
    "    \n",
    "    def classify_session(timestamps):\n",
    "        try:\n",
    "            # 1. 使用 .dt.tz 來檢查時區信息\n",
    "            timestamps = timestamps.dt.tz_localize(None) if timestamps.dt.tz is not None else timestamps\n",
    "            \n",
    "            # 2. 使用 .dt.time 和 .dt.date 來提取時間和日期部分\n",
    "            times = timestamps.dt.time\n",
    "            dates = timestamps.dt.date\n",
    "\n",
    "            # 初始化結果\n",
    "            session_type = pd.Series(\"other\", index=timestamps.index)\n",
    "            session_start = pd.Series(pd.NaT, index=timestamps.index)\n",
    "        \n",
    "            # 日盤條件\n",
    "            day_mask = (times >= t1) & (times <= t2)\n",
    "            session_type.loc[day_mask] = \"day\"\n",
    "            session_start.loc[day_mask] = pd.to_datetime(\n",
    "                dates[day_mask].astype(str) + \" \" + t1.strftime(\"%H:%M:%S\")\n",
    "            )\n",
    "    \n",
    "            # 夜盤條件 (當天夜盤)\n",
    "            night_mask1 = (times >= t4) & ~day_mask\n",
    "            session_type.loc[night_mask1] = \"night\"\n",
    "            session_start.loc[night_mask1] = pd.to_datetime(\n",
    "                dates[night_mask1].astype(str) + \" \" + t4.strftime(\"%H:%M:%S\")\n",
    "            )\n",
    "        \n",
    "            # 夜盤條件 (前一天夜盤，時間 < t4)\n",
    "            night_mask2 = (times < t4) & ~day_mask\n",
    "            session_type.loc[night_mask2] = \"night\"\n",
    "            prev_dates = (timestamps[night_mask2] - timedelta(days=1)).dt.date\n",
    "            session_start.loc[night_mask2] = pd.to_datetime(\n",
    "                prev_dates.astype(str) + \" \" + t4.strftime(\"%H:%M:%S\")\n",
    "            )\n",
    "\n",
    "            return pd.DataFrame({\"session_type\": session_type, \"session_start\": session_start})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"時間轉換出現錯誤: {e}\")\n",
    "            return pd.DataFrame({\"session_type\": None, \"session_start\": None}, index=timestamps.index)\n",
    "    \n",
    "    result = classify_session(df.index.to_series())\n",
    "    df.loc[:, [\"session_type\", \"session_start\"]] = result[[\"session_type\", \"session_start\"]]\n",
    "    df = df[df[\"session_type\"].isin([\"day\", \"night\"])]\n",
    "\n",
    "    # 新增4: 補齊缺失的1分K資料\n",
    "    def fill_missing_minutes(df_session, session_start, session_type):\n",
    "        # 定義交易時段範圍\n",
    "        if session_type == \"day\":\n",
    "            start_time = datetime.combine(session_start.date(), t1)\n",
    "            end_time = datetime.combine(session_start.date(), t2)\n",
    "        else:  # night\n",
    "            start_time = datetime.combine(session_start.date(), t4)\n",
    "            end_time = datetime.combine(session_start.date() + timedelta(days=1), t3)\n",
    "\n",
    "        # 生成完整的1分鐘時間序列\n",
    "        full_time_index = pd.date_range(start=start_time, end=end_time, freq=\"1min\")\n",
    "        existing_times = df_session.index\n",
    "\n",
    "        # 找出缺失的時間點\n",
    "        missing_times = [t for t in full_time_index if t not in existing_times]\n",
    "        \n",
    "        if missing_times:\n",
    "            # 為每個缺失時間點填充資料\n",
    "            missing_data = []\n",
    "            last_valid_row = None\n",
    "            for t in missing_times:\n",
    "                # 找到前一筆有效資料\n",
    "                prev_time = t - timedelta(minutes=1)\n",
    "                if prev_time in df_session.index:\n",
    "                    last_valid_row = df_session.loc[prev_time]\n",
    "                if last_valid_row is not None:\n",
    "                    missing_data.append({\n",
    "                        \"ts\": t,\n",
    "                        \"open\": last_valid_row[\"close\"],\n",
    "                        \"high\": last_valid_row[\"close\"],\n",
    "                        \"low\": last_valid_row[\"close\"],\n",
    "                        \"close\": last_valid_row[\"close\"],\n",
    "                        \"volume\": 0,\n",
    "                        \"amount\": 0,\n",
    "                        \"complete\": True,\n",
    "                        \"session_type\": session_type,\n",
    "                        \"session_start\": session_start\n",
    "                    })\n",
    "\n",
    "            # 將缺失資料合併到原資料\n",
    "            if missing_data:\n",
    "                missing_df = pd.DataFrame(missing_data).set_index(\"ts\")\n",
    "                df_session = pd.concat([df_session, missing_df]).sort_index()\n",
    "\n",
    "        return df_session\n",
    "\n",
    "    # 按 session 分組並補齊缺失資料\n",
    "    df_filled = []\n",
    "    for session_start, session_data in df.groupby(\"session_start\"):\n",
    "        session_type = session_data[\"session_type\"].iloc[0]\n",
    "        session_data = fill_missing_minutes(session_data, session_start, session_type)\n",
    "        df_filled.append(session_data)\n",
    "        \n",
    "    if df_filled:\n",
    "        df = pd.concat(df_filled).sort_index()\n",
    "\n",
    "    df.index = df.index - pd.Timedelta(minutes=1)\n",
    "\n",
    "    # 設定 K 棒時間長度\n",
    "    window = timedelta(minutes=freq)\n",
    "\n",
    "    # 分段處理每個 session 的資料\n",
    "    result = []\n",
    "\n",
    "    for session_start, session_data in df.groupby(\"session_start\"):\n",
    "        current_time = session_start\n",
    "        max_time = session_data.index.max()\n",
    "        \n",
    "        # 对 session 进行累积计算\n",
    "        if not session_data.empty:\n",
    "            session_data[\"acc_vol\"] = session_data[\"volume\"].cumsum()  # 在 session 内累积 volume\n",
    "            session_data[\"acc_price_volume\"] = session_data[\"sum_price_volume\"].cumsum()  # 累积 sum_price_volume\n",
    "            session_data[\"acc_price_sq_volume\"] = session_data[\"sum_price_sq_volume\"].cumsum()  # 累积 sum_price_sq_volume\n",
    "\n",
    "        while current_time < max_time:\n",
    "            next_time = current_time + window\n",
    "            window_data = session_data[(session_data.index >= current_time) & (session_data.index < next_time)]\n",
    "\n",
    "            if not window_data.empty:\n",
    "                o = window_data[\"open\"].iloc[0]\n",
    "                h = window_data[\"high\"].max()\n",
    "                l = window_data[\"low\"].min()\n",
    "                c = window_data[\"close\"].iloc[-1]\n",
    "                v = window_data[\"volume\"].sum()\n",
    "                a = c * v\n",
    "                complete = window_data.index[-1] >= next_time - timedelta(minutes=1)\n",
    "                \n",
    "                # 添加额外的列聚合\n",
    "                flow_imbalance_agg = window_data['flow_imbalance'].sum()\n",
    "                avg_spread_agg = window_data['avg_spread'].sum() / v if v != 0 else 0\n",
    "                avg_obi_agg = window_data['avg_obi'].sum() / v if v != 0 else 0.5\n",
    "                sum_price_volume = window_data['sum_price_volume'].sum()\n",
    "                sum_price_sq_volume = window_data['sum_price_sq_volume'].sum()\n",
    "                monetary_delta_agg = window_data['monetary_delta'].sum()\n",
    "                volume_delta_agg = window_data['volume_delta'].sum()\n",
    "                pv_list_agg = aggregate_pv_list(window_data['pv_list'])\n",
    "                acc_vol = window_data[\"acc_vol\"].sum()\n",
    "                acc_price_vol = window_data[\"acc_price_volume\"].sum()\n",
    "                acc_price_sq_volume = window_data[\"acc_price_sq_volume\"].sum()\n",
    "\n",
    "                result.append({\n",
    "                    \"ts\": current_time,\n",
    "                    \"open\": o,\n",
    "                    \"high\": h,\n",
    "                    \"low\": l,\n",
    "                    \"close\": c,\n",
    "                    \"volume\": v,\n",
    "                    \"amount\": a,\n",
    "                    \"flow_imbalance\": flow_imbalance_agg,\n",
    "                    \"avg_spread\": avg_spread_agg,\n",
    "                    \"avg_obi\": avg_obi_agg,\n",
    "                    \"sum_price_volume\": sum_price_volume,\n",
    "                    \"sum_price_sq_volume\": sum_price_sq_volume,\n",
    "                    \"monetary_delta\": monetary_delta_agg,\n",
    "                    \"volume_delta\": volume_delta_agg,\n",
    "                    \"pv_list\": pv_list_agg,\n",
    "                    \"acc_volume\": acc_vol,\n",
    "                    \"acc_price_volume\": acc_price_vol,\n",
    "                    \"acc_price_sq_volume\": acc_price_sq_volume,\n",
    "                    \"complete\": complete\n",
    "                })\n",
    "\n",
    "            current_time = next_time\n",
    "            \n",
    "    # 建立新的 DataFrame\n",
    "    agg_df = pd.DataFrame(result)\n",
    "    agg_df.set_index(\"ts\", inplace=True, drop=False)\n",
    "    agg_df = agg_df.shift(1).dropna()\n",
    "\n",
    "    return agg_df\n",
    "\n",
    "def combine_daily_k_bars(df):\n",
    "    df = df.copy()\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df['date'] = df.index.date\n",
    "    df['time'] = df.index.time\n",
    "    \n",
    "    combined_data = []\n",
    "    \n",
    "    for date, group in df.groupby('date'):\n",
    "        aggregated_row = {\n",
    "            'date': date,\n",
    "            'open': group['open'].iloc[0],\n",
    "            'high': group['high'].max(),\n",
    "            'low': group['low'].min(),\n",
    "            'close': group['close'].iloc[-1],\n",
    "            'volume': group['volume'].sum(),\n",
    "            'complete': True,\n",
    "            'flow_imbalance': group['flow_imbalance'].sum(),\n",
    "            'monetary_delta': group['monetary_delta'].sum(),\n",
    "            'volume_delta': group['volume_delta'].sum(),\n",
    "            'acc_price_volume': group['acc_price_volume'].iloc[-1],\n",
    "            'acc_price_sq_volume': group['acc_price_sq_volume'].iloc[-1],\n",
    "            'acc_volume': group['acc_volume'].iloc[-1],\n",
    "            'amount': group['amount'].sum()\n",
    "        }\n",
    "        \n",
    "        combined_data.append(aggregated_row)\n",
    "    \n",
    "    # 创建结果DataFrame\n",
    "    combined = pd.DataFrame(combined_data)\n",
    "    combined['ts'] = pd.to_datetime(combined['date'])\n",
    "    combined = combined.set_index('ts')\n",
    "    combined = combined.drop('date', axis=1)\n",
    "    \n",
    "    return combined.shift(1).dropna()\n",
    "\n",
    "def process_multiple_datasets_by_second(dataset1, dataset2, codes):\n",
    "    dfs_by_code = {code: [] for code in codes}\n",
    "    dfk_by_code = {code: [] for code in codes}\n",
    "    extra_by_code = {code: [] for code in codes}\n",
    "    \n",
    "    # 處理 tick 資料\n",
    "    for type_of_data, data_name, date_start, date_end, data_type, *params in dataset1:\n",
    "        origin_df = prepare_data(type_of_data, data_name)\n",
    "        df, extra_data = pre_process_by_second(origin_df, data_type, date_start, date_end)\n",
    "        \n",
    "        for code in codes:\n",
    "            if data_name.startswith(code):\n",
    "                dfs_by_code[code].append(df)\n",
    "                extra_by_code[code].append(extra_data)\n",
    "                break\n",
    "\n",
    "    # 處理 1分K 資料\n",
    "    for type_of_data, data_name in dataset2:\n",
    "        df = prepare_data(type_of_data, data_name).set_index('ts')\n",
    "        \n",
    "        for code in codes:\n",
    "            if data_name.startswith(code + 'k'):\n",
    "                dfk_by_code[code].append(df)\n",
    "                break\n",
    "\n",
    "    # 合併各商品的 dataframe\n",
    "    final_dfs = {}\n",
    "    final_dfks = {}\n",
    "\n",
    "    for code in codes:\n",
    "        # 合併同一商品的多個資料段\n",
    "        df = pd.concat(dfs_by_code[code]) if dfs_by_code[code] else pd.DataFrame()\n",
    "        dfk = pd.concat(dfk_by_code[code]) if dfk_by_code[code] else pd.DataFrame()\n",
    "        extra = pd.concat(extra_by_code[code]) if extra_by_code[code] else pd.DataFrame()\n",
    "\n",
    "        # groupby 處理重複時間\n",
    "        if not df.empty:\n",
    "            df = df.groupby(df.index).last().sort_index().dropna()\n",
    "        if not dfk.empty:\n",
    "            dfk = dfk.groupby(dfk.index).last().sort_index().dropna()\n",
    "\n",
    "        # 1k 資料加上 extra 資料（若存在）\n",
    "        if not dfk.empty and not extra.empty:\n",
    "            dfk = pd.merge(dfk, extra, left_index=True, right_index=True, how=\"inner\")\n",
    "\n",
    "        final_dfs[code] = df\n",
    "        final_dfks[code] = dfk\n",
    "\n",
    "    # === 合併所有商品成一張 df（依 index inner join）===\n",
    "    merged_df = None\n",
    "    for code in codes:\n",
    "        if final_dfs[code].empty:\n",
    "            continue\n",
    "        if merged_df is None:\n",
    "            merged_df = final_dfs[code].copy()\n",
    "            merged_df = merged_df.add_suffix(f'_{code}')\n",
    "        else:\n",
    "            df_tmp = final_dfs[code].add_suffix(f'_{code}')\n",
    "            merged_df = pd.merge(merged_df, df_tmp, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "    return merged_df, final_dfks\n",
    "\n",
    "def process_multiple_datasets_by_custom(dataset1, codes):\n",
    "    # 建立每個 bar 類型的 dict\n",
    "    bar_types = [\"tick\", \"vol\", \"time\", \"sum\", \"dollar\", \"range\", \"renko\"]\n",
    "    bar_dict = {btype: {code: [] for code in codes} for btype in bar_types}\n",
    "\n",
    "    # 處理 tick 資料\n",
    "    for type_of_data, data_name, _, _, _, tick_bar, volume_bar, time_bar, sum_bar, dollar_bar, range_bar, renko_bar in dataset1:\n",
    "        origin_df = prepare_data(type_of_data, data_name)\n",
    "\n",
    "        tick_data   = pre_process_by_finmlkit_tickbar(origin_df, tick_bar.get('n', 50))\n",
    "        volume_data = pre_process_by_finmlkit_volbar(origin_df, volume_bar.get('volume_threshold', 1000))\n",
    "        time_data   = pre_process_by_finmlkit_timebar(origin_df, time_bar.get('time', '1min'))\n",
    "        sum_data    = pre_process_by_finmlkit_sumbar(origin_df, sum_bar.get('window', 200), sum_bar.get('min_sigma', 5e-4), sum_bar.get('sigma_mult', 2))\n",
    "        dollar_data = pre_process_by_finmlkit_dollarbar(origin_df, dollar_bar.get('dollar_threshold', 1_000_000))\n",
    "        range_data  = pre_process_by_range(origin_df, range_bar.get('range_size', 0.005))\n",
    "        renko_data  = pre_process_by_renko(origin_df, renko_bar.get('brick_size', 0.5), renko_bar.get('type', 'normal'))\n",
    "\n",
    "        for code in codes:\n",
    "            if data_name.startswith(code):\n",
    "                bar_dict[\"tick\"][code].append(tick_data)\n",
    "                bar_dict[\"vol\"][code].append(volume_data)\n",
    "                bar_dict[\"time\"][code].append(time_data)\n",
    "                bar_dict[\"sum\"][code].append(sum_data)\n",
    "                bar_dict[\"dollar\"][code].append(dollar_data)\n",
    "                bar_dict[\"range\"][code].append(range_data)\n",
    "                bar_dict[\"renko\"][code].append(renko_data)\n",
    "                break\n",
    "\n",
    "    # === 這裡不 concat，只是簡單複製 === #\n",
    "    final_bars = bar_dict\n",
    "\n",
    "    # === 合併 range / renko 成 DataFrame === #\n",
    "    def merge_multi_bars(bar_data_dict):\n",
    "        non_empty = [(code, df) for code, df in bar_data_dict.items() if not isinstance(df, list) and not df.empty]\n",
    "        if not non_empty:\n",
    "            return pd.DataFrame()\n",
    "        merged = reduce(\n",
    "            lambda left, right: pd.merge(\n",
    "                left[1].add_suffix(f\"_{left[0]}\"),\n",
    "                right[1].add_suffix(f\"_{right[0]}\"),\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "                how='outer'\n",
    "            ),\n",
    "            non_empty\n",
    "        )\n",
    "        merged = merged.sort_index().ffill().dropna()\n",
    "        return merged\n",
    "\n",
    "    # 因為 range/renko 的 bar 是 DataFrame（不是 finmlkit 物件）\n",
    "    df_rangebar = merge_multi_bars(final_bars[\"range\"])\n",
    "    df_renkobar = merge_multi_bars(final_bars[\"renko\"])\n",
    "\n",
    "    return df_rangebar, df_renkobar, final_bars\n",
    "\n",
    "CODE = ['2349', '3050', '8104']\n",
    "session = 's_day'\n",
    "params1 = {\n",
    "    'tick_bar': {'n': 120},\n",
    "    'volume_bar': {'volume_threshold': 150},\n",
    "    'time_bar': {'time': '60min'},\n",
    "    'sum_bar': {'window': 200, 'min_sigma': 7.656013e-05, 'sigma_mult': 3.791},\n",
    "    'dollar_bar': {'dollar_threshold': 100000},\n",
    "    'range_bar': {'range_size': 0.007},\n",
    "    'renko_bar': {'brick_size': 1}\n",
    "}\n",
    "\n",
    "params2 = {\n",
    "    'tick_bar': {'n': 5},\n",
    "    'volume_bar': {'volume_threshold': 10},\n",
    "    'time_bar': {'time': '60min'},\n",
    "    'sum_bar': {'window': 200, 'min_sigma': 4.003475e-04, 'sigma_mult': 4.061},\n",
    "    'dollar_bar': {'dollar_threshold': 35000},\n",
    "    'range_bar': {'range_size': 0.005},\n",
    "    'renko_bar': {'brick_size': 2.5}\n",
    "}\n",
    "\n",
    "params3 = {\n",
    "    'tick_bar': {'n': 5},\n",
    "    'volume_bar': {'volume_threshold': 10},\n",
    "    'time_bar': {'time': '60min'},\n",
    "    'sum_bar': {'window': 200, 'min_sigma': 4.003475e-04, 'sigma_mult': 4.061},\n",
    "    'dollar_bar': {'dollar_threshold': 35000},\n",
    "    'range_bar': {'range_size': 0.005},\n",
    "    'renko_bar': {'brick_size': 2.5}\n",
    "}\n",
    "\n",
    "dataset1 = [\n",
    "    ('shioaji/2024_0111', f'{CODE[0]}', '2024-01-02 9:00:00', '2025-11-10 13:30:00', f'{session}', *params1.values()),\n",
    "    ('shioaji/2024_0111', f'{CODE[1]}', '2024-01-02 9:00:00', '2025-11-10 13:30:00', f'{session}', *params2.values()),\n",
    "    ('shioaji/2024_0111', f'{CODE[2]}', '2024-01-02 9:00:00', '2025-11-10 13:30:00', f'{session}', *params3.values()),\n",
    "]\n",
    "\n",
    "dataset2 = [\n",
    "    ('shioaji/2024_0111', f'{CODE[0]}k'),\n",
    "    ('shioaji/2024_0111', f'{CODE[1]}k'),\n",
    "    ('shioaji/2024_0111', f'{CODE[2]}k'),\n",
    "]\n",
    "\n",
    "# 放入 (貴的, 便宜的), 數值大的在ratio分母, 使得數值可以相除在0~1\n",
    "df, dfk = process_multiple_datasets_by_second(dataset1, dataset2, CODE)\n",
    "#df_range, df_renko, final_bars = process_multiple_datasets_by_custom(dataset1, CODE)\n",
    "#df_tick_k, df_tick_direct, df_tick_size = concate_finmlkit(final_bars[\"tick\"], 'tick')\n",
    "#df_vol_k, df_vol_direct, df_vol_size = concate_finmlkit(final_bars[\"vol\"], 'volume')\n",
    "#df_time_k, df_time_direct, df_time_size = concate_finmlkit(final_bars[\"time\"], 'time')\n",
    "#df_sum_k, df_sum_direct, df_sum_size = concate_finmlkit(final_bars[\"sum\"], 'sum')\n",
    "#df_dollar_k, df_dollar_direct, df_dollar_size = concate_finmlkit(final_bars[\"dollar\"], 'dollar')\n",
    "\n",
    "print(\"Index:\", df.head(3).index.tolist())\n",
    "print(\"Columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8471d4f-c650-4d53-bcd0-b1af8ffdb543",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_time = 1440\n",
    "k = {code: combine_daily_k_bars(convert_ohlcv(dfk[code], K_time)) for code in CODE}\n",
    "\n",
    "dfs = []\n",
    "for code, df in k.items():\n",
    "    df_renamed = df.copy()\n",
    "    df_renamed.columns = [f\"{col}_{code}\" for col in df_renamed.columns]\n",
    "    \n",
    "    # 取出需要的原始序列\n",
    "    close = df_renamed[f'close_{code}']\n",
    "    volume = df_renamed[f'volume_{code}']\n",
    "    \n",
    "    # 累積計算\n",
    "    acc_vol = volume.cumsum() # ΣV\n",
    "    acc_pv = (close * volume).cumsum() # Σ(P×V)\n",
    "    acc_p2v = (close**2 * volume).cumsum() # Σ(P²×V)\n",
    "\n",
    "    vwap = acc_pv / acc_vol\n",
    "    vwap_variance = acc_p2v / acc_vol - vwap**2\n",
    "    vwap_std = np.sqrt(vwap_variance.clip(lower=0))\n",
    "    \n",
    "    df_renamed[f'volume_acc_{code}'] = acc_vol\n",
    "    df_renamed[f'vwap_{code}'] = vwap\n",
    "    df_renamed[f'vwap_std_{code}'] = vwap_std\n",
    "    \n",
    "    dfs.append(df_renamed)\n",
    "\n",
    "df_all = pd.concat(dfs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b5351b-c0b2-4265-abb1-81659c5376b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circle_omega(series):\n",
    "    # 想知道「現在量是在升還是降」 → 看 Y\n",
    "    # 想知道「變化速度快不快」 → 看 ω\n",
    "    # 想知道「是不是即將轉折」 → 看 κ\n",
    "    # 想知道「循環位置」 → 看 θ\n",
    "\n",
    "    ma = talib.EMA(series, params.get('indicator_param')).fillna(0)\n",
    "    X = ma - ma.mean() # 位移\n",
    "    Y = ma.diff().fillna(0) # 速度\n",
    " \n",
    "    # 3. 計算相位角 θ(t)\n",
    "    theta = np.arctan2(Y, X)\n",
    "\n",
    "    # 4. 角速度 ω(t) = dθ/dt\n",
    "    dtheta = np.diff(theta)\n",
    "    dtheta = np.unwrap(dtheta) # 避免跳角度\n",
    "    omega = pd.Series(dtheta)\n",
    "\n",
    "    # 5. 曲率 κ（反映「加速度變化」）\n",
    "    V = np.sqrt(X**2 + Y**2) # \"速度向量大小\"（非價格漲跌）\n",
    "    dX = X.diff().fillna(0)\n",
    "    dY = Y.diff().fillna(0)\n",
    "\n",
    "    # 平面曲率公式 κ = |x'y'' – y'x''| / ( (x'^2 + y'^2)^(3/2) )\n",
    "    curvature_num = (Y.iloc[-2] * dX.iloc[-1] - X.iloc[-2] * dY.iloc[-1])\n",
    "    curvature_den = ((Y.iloc[-2]**2 + X.iloc[-2]**2)**1.5 + 1e-9)\n",
    "    curvature = curvature_num / curvature_den\n",
    "    \n",
    "    return {\n",
    "        \"X\": X.iloc[-1],\n",
    "        \"Y\": Y.iloc[-1],\n",
    "        \"theta\": theta.iloc[-1], # 當前相位角\n",
    "        \"omega\": omega.iloc[-1], # 角速度\n",
    "        \"curvature\": curvature # 曲率（加速度變化）\n",
    "    }\n",
    "\n",
    "def bias_ratio(series, period):\n",
    "    ma = talib.EMA(series, period)\n",
    "    bias_ratio = (series - ma) / ma * 100\n",
    "    return bias_ratio.iloc[-1]\n",
    "\n",
    "def corr_signal(df_all, **params):\n",
    "    k_lookback = params.get('k_lookback')\n",
    "    indicator = params.get('indicator')\n",
    "    col = params.get('col')\n",
    "    codes = params.get('codes')\n",
    "    results = []\n",
    "       \n",
    "    for i in range(k_lookback - 1):\n",
    "        results.append({'ts': df_all.index[i], **{code: np.nan for code in codes}})\n",
    "        \n",
    "    for i in range(0, len(df_all) - k_lookback + 1):\n",
    "        if i + k_lookback > len(df_all):\n",
    "            break\n",
    "          \n",
    "        df_subset = df_all.iloc[i:i + k_lookback].copy()\n",
    "        \n",
    "        indicators = {}\n",
    "        for code in codes:\n",
    "            if indicator == 'bias':\n",
    "                series = df_subset[f'{col}_{code}'].dropna()\n",
    "                indicators[code] = bias_ratio(series, params.get('indicator_param'))\n",
    "            \n",
    "            if indicator == 'ov':\n",
    "                series = df_subset[f'{col}_{code}']\n",
    "                indicators[code] = circle_omega(series)['theta']\n",
    "                \n",
    "        score_row = dict(indicators)\n",
    "        score_row['ts'] = df_subset.index[-1]\n",
    "        results.append(score_row)\n",
    "    \n",
    "    result_df = pd.DataFrame(results).set_index('ts')\n",
    "    return result_df.dropna()\n",
    "\n",
    "params = {\n",
    "    'k_lookback': 20,\n",
    "    'col': 'vwap',\n",
    "    'indicator_param': 5,\n",
    "    'indicator': 'bias',\n",
    "    'codes': CODE\n",
    "}\n",
    "bias = corr_signal(df_all, **params)\n",
    "\n",
    "params = {\n",
    "    'k_lookback': 20,\n",
    "    'col': 'volume_acc',\n",
    "    'indicator_param': 5,\n",
    "    'indicator': 'ov',\n",
    "    'codes': CODE\n",
    "}\n",
    "ov = corr_signal(df_all, **params)\n",
    "\n",
    "# 1. 加上後綴區分\n",
    "bias_renamed = bias.add_suffix('_bias')\n",
    "ov_renamed = ov.add_suffix('_ov')\n",
    "\n",
    "# 2. 合併\n",
    "df_merged = pd.concat([bias_renamed, ov_renamed], axis=1)\n",
    "df_final = pd.concat([df_all, df_merged], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dba3adad-d8b9-489e-8dbb-89e7e0b50f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在分析窗口：2024-01-31 00:00:00 to 2024-02-20 00:00:00...\n",
      "完成窗口：2024-01-31 00:00:00 to 2024-02-20 00:00:00\n",
      "正在分析窗口：2024-02-01 00:00:00 to 2024-02-21 00:00:00...\n",
      "完成窗口：2024-02-01 00:00:00 to 2024-02-21 00:00:00\n",
      "正在分析窗口：2024-02-02 00:00:00 to 2024-02-22 00:00:00...\n",
      "完成窗口：2024-02-02 00:00:00 to 2024-02-22 00:00:00\n",
      "正在分析窗口：2024-02-05 00:00:00 to 2024-02-23 00:00:00...\n",
      "完成窗口：2024-02-05 00:00:00 to 2024-02-23 00:00:00\n",
      "正在分析窗口：2024-02-15 00:00:00 to 2024-02-26 00:00:00...\n",
      "完成窗口：2024-02-15 00:00:00 to 2024-02-26 00:00:00\n",
      "正在分析窗口：2024-02-16 00:00:00 to 2024-02-27 00:00:00...\n",
      "完成窗口：2024-02-16 00:00:00 to 2024-02-27 00:00:00\n",
      "正在分析窗口：2024-02-19 00:00:00 to 2024-02-29 00:00:00...\n",
      "完成窗口：2024-02-19 00:00:00 to 2024-02-29 00:00:00\n",
      "正在分析窗口：2024-02-20 00:00:00 to 2024-03-01 00:00:00...\n",
      "完成窗口：2024-02-20 00:00:00 to 2024-03-01 00:00:00\n",
      "正在分析窗口：2024-02-21 00:00:00 to 2024-03-04 00:00:00...\n",
      "完成窗口：2024-02-21 00:00:00 to 2024-03-04 00:00:00\n",
      "正在分析窗口：2024-02-22 00:00:00 to 2024-03-05 00:00:00...\n",
      "完成窗口：2024-02-22 00:00:00 to 2024-03-05 00:00:00\n",
      "正在分析窗口：2024-02-23 00:00:00 to 2024-03-06 00:00:00...\n",
      "完成窗口：2024-02-23 00:00:00 to 2024-03-06 00:00:00\n",
      "正在分析窗口：2024-02-26 00:00:00 to 2024-03-07 00:00:00...\n",
      "完成窗口：2024-02-26 00:00:00 to 2024-03-07 00:00:00\n",
      "正在分析窗口：2024-02-27 00:00:00 to 2024-03-08 00:00:00...\n",
      "完成窗口：2024-02-27 00:00:00 to 2024-03-08 00:00:00\n",
      "正在分析窗口：2024-02-29 00:00:00 to 2024-03-11 00:00:00...\n",
      "完成窗口：2024-02-29 00:00:00 to 2024-03-11 00:00:00\n",
      "正在分析窗口：2024-03-01 00:00:00 to 2024-03-12 00:00:00...\n",
      "完成窗口：2024-03-01 00:00:00 to 2024-03-12 00:00:00\n",
      "正在分析窗口：2024-03-04 00:00:00 to 2024-03-13 00:00:00...\n",
      "完成窗口：2024-03-04 00:00:00 to 2024-03-13 00:00:00\n",
      "正在分析窗口：2024-03-05 00:00:00 to 2024-03-14 00:00:00...\n",
      "完成窗口：2024-03-11 00:00:00 to 2024-03-20 00:00:00\n",
      "正在分析窗口：2024-03-12 00:00:00 to 2024-03-21 00:00:00...\n",
      "完成窗口：2024-03-12 00:00:00 to 2024-03-21 00:00:00\n",
      "正在分析窗口：2024-03-13 00:00:00 to 2024-03-22 00:00:00...\n",
      "完成窗口：2024-03-13 00:00:00 to 2024-03-22 00:00:00\n",
      "正在分析窗口：2024-03-14 00:00:00 to 2024-03-25 00:00:00...\n",
      "完成窗口：2024-03-14 00:00:00 to 2024-03-25 00:00:00\n",
      "正在分析窗口：2024-03-15 00:00:00 to 2024-03-26 00:00:00...\n",
      "完成窗口：2024-03-15 00:00:00 to 2024-03-26 00:00:00\n",
      "正在分析窗口：2024-03-18 00:00:00 to 2024-03-27 00:00:00...\n",
      "完成窗口：2024-03-18 00:00:00 to 2024-03-27 00:00:00\n",
      "正在分析窗口：2024-03-19 00:00:00 to 2024-03-28 00:00:00...\n",
      "完成窗口：2024-03-19 00:00:00 to 2024-03-28 00:00:00\n",
      "正在分析窗口：2024-03-20 00:00:00 to 2024-03-29 00:00:00...\n",
      "完成窗口：2024-03-20 00:00:00 to 2024-03-29 00:00:00\n",
      "正在分析窗口：2024-03-21 00:00:00 to 2024-04-01 00:00:00...\n",
      "完成窗口：2024-03-21 00:00:00 to 2024-04-01 00:00:00\n",
      "正在分析窗口：2024-03-22 00:00:00 to 2024-04-02 00:00:00...\n",
      "完成窗口：2024-03-22 00:00:00 to 2024-04-02 00:00:00\n",
      "正在分析窗口：2024-03-25 00:00:00 to 2024-04-03 00:00:00...\n",
      "完成窗口：2024-03-25 00:00:00 to 2024-04-03 00:00:00\n",
      "正在分析窗口：2024-03-26 00:00:00 to 2024-04-08 00:00:00...\n",
      "完成窗口：2024-03-26 00:00:00 to 2024-04-08 00:00:00\n",
      "正在分析窗口：2024-03-27 00:00:00 to 2024-04-09 00:00:00...\n",
      "完成窗口：2024-03-27 00:00:00 to 2024-04-09 00:00:00\n",
      "正在分析窗口：2024-03-28 00:00:00 to 2024-04-10 00:00:00...\n",
      "完成窗口：2024-03-28 00:00:00 to 2024-04-10 00:00:00\n",
      "正在分析窗口：2024-03-29 00:00:00 to 2024-04-11 00:00:00...\n",
      "完成窗口：2024-03-29 00:00:00 to 2024-04-11 00:00:00\n",
      "正在分析窗口：2024-04-01 00:00:00 to 2024-04-12 00:00:00...\n",
      "完成窗口：2024-04-01 00:00:00 to 2024-04-12 00:00:00\n",
      "正在分析窗口：2024-04-02 00:00:00 to 2024-04-15 00:00:00...\n",
      "完成窗口：2024-04-02 00:00:00 to 2024-04-15 00:00:00\n",
      "正在分析窗口：2024-04-03 00:00:00 to 2024-04-16 00:00:00...\n",
      "完成窗口：2024-04-03 00:00:00 to 2024-04-16 00:00:00\n",
      "正在分析窗口：2024-04-08 00:00:00 to 2024-04-17 00:00:00...\n",
      "完成窗口：2024-04-08 00:00:00 to 2024-04-17 00:00:00\n",
      "正在分析窗口：2024-04-09 00:00:00 to 2024-04-18 00:00:00...\n",
      "完成窗口：2024-04-09 00:00:00 to 2024-04-18 00:00:00\n",
      "正在分析窗口：2024-04-10 00:00:00 to 2024-04-19 00:00:00...\n",
      "完成窗口：2024-04-10 00:00:00 to 2024-04-19 00:00:00\n",
      "正在分析窗口：2024-04-11 00:00:00 to 2024-04-22 00:00:00...\n",
      "完成窗口：2024-04-11 00:00:00 to 2024-04-22 00:00:00\n",
      "正在分析窗口：2024-04-12 00:00:00 to 2024-04-23 00:00:00...\n",
      "完成窗口：2024-04-12 00:00:00 to 2024-04-23 00:00:00\n",
      "正在分析窗口：2024-04-15 00:00:00 to 2024-04-24 00:00:00...\n",
      "完成窗口：2024-04-15 00:00:00 to 2024-04-24 00:00:00\n",
      "正在分析窗口：2024-04-16 00:00:00 to 2024-04-25 00:00:00...\n",
      "完成窗口：2024-04-16 00:00:00 to 2024-04-25 00:00:00\n",
      "正在分析窗口：2024-04-17 00:00:00 to 2024-04-26 00:00:00...\n",
      "完成窗口：2024-04-17 00:00:00 to 2024-04-26 00:00:00\n",
      "正在分析窗口：2024-04-18 00:00:00 to 2024-04-29 00:00:00...\n",
      "完成窗口：2024-04-18 00:00:00 to 2024-04-29 00:00:00\n",
      "正在分析窗口：2024-04-19 00:00:00 to 2024-04-30 00:00:00...\n",
      "完成窗口：2024-04-19 00:00:00 to 2024-04-30 00:00:00\n",
      "正在分析窗口：2024-04-22 00:00:00 to 2024-05-02 00:00:00...\n",
      "完成窗口：2024-04-22 00:00:00 to 2024-05-02 00:00:00\n",
      "正在分析窗口：2024-04-23 00:00:00 to 2024-05-03 00:00:00...\n",
      "完成窗口：2024-04-23 00:00:00 to 2024-05-03 00:00:00\n",
      "正在分析窗口：2024-04-24 00:00:00 to 2024-05-06 00:00:00...\n",
      "完成窗口：2024-04-24 00:00:00 to 2024-05-06 00:00:00\n",
      "正在分析窗口：2024-04-25 00:00:00 to 2024-05-07 00:00:00...\n",
      "完成窗口：2024-04-25 00:00:00 to 2024-05-07 00:00:00\n",
      "正在分析窗口：2024-04-26 00:00:00 to 2024-05-08 00:00:00...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 287\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m預處理完成，所有總結存於\u001b[39m\u001b[38;5;124m\"\u001b[39m, json_file)\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summaries\n\u001b[0;32m--> 287\u001b[0m summaries \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCODE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mJSON_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_lookback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 262\u001b[0m, in \u001b[0;36mprocess_windows\u001b[0;34m(df_all, CODE, json_file, k_lookback, N)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m     input_prompt \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124m        請分析以下時間窗口的數據：\u001b[39m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124m        **分析期**： \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124m        請根據以上數據，使用「分析時間段分析出合適的資料」並依照規定進行輸出。\u001b[39m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m--> 262\u001b[0m     trend_raw \u001b[38;5;241m=\u001b[39m \u001b[43mmarket_trend\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrend tool error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 194\u001b[0m, in \u001b[0;36mmarket_trend\u001b[0;34m(input_text)\u001b[0m\n\u001b[1;32m    189\u001b[0m chat_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages([\n\u001b[1;32m    190\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, system_prompt_step1),\n\u001b[1;32m    191\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{input_data}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 使用變量占位符\u001b[39;00m\n\u001b[1;32m    192\u001b[0m ])\n\u001b[1;32m    193\u001b[0m chain \u001b[38;5;241m=\u001b[39m chat_prompt \u001b[38;5;241m|\u001b[39m llm\n\u001b[0;32m--> 194\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m summary \u001b[38;5;241m=\u001b[39m clean_think_content(response\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m summary\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:3246\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3244\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3245\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3246\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3247\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:395\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    392\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 395\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    405\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1025\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1023\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1024\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:842\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    841\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 842\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m         )\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    850\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1091\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1091\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1095\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1172\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[1;32m   1166\u001b[0m             response,\n\u001b[1;32m   1167\u001b[0m             schema\u001b[38;5;241m=\u001b[39moriginal_schema_obj,\n\u001b[1;32m   1168\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m   1169\u001b[0m             output_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_version,\n\u001b[1;32m   1170\u001b[0m         )\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1172\u001b[0m         raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1173\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/openai/_utils/_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[1;32m   1145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/openai/_base_client.py:982\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 982\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    988\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/langchain/lib/python3.10/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json, os, random, re, textwrap\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "JSON_FILE = f'./history.json'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # 你的本地服務器地址\n",
    "    api_key=\"sk-no-key-required\",  # 如果不需要 API key，用占位符\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",  # 你的 Hugging Face 模型名稱\n",
    "    temperature=0  # 根據需求調整\n",
    ")\n",
    "\n",
    "system_prompt_step1 = textwrap.dedent(\"\"\"\n",
    "    你是一位專業的量化分析師，任務是分析多個商品的時間序列數據，判斷每個商品在分析期內的盤勢狀況。\n",
    "    對每一個輸入的商品，**完全獨立、逐筆進行深度分析**，嚴禁將任何一筆商品的走勢套用到其他商品，即使形態類似，也必須指出差異。\n",
    "\n",
    "    【數據說明】\n",
    "    - 每個商品的數據是時間序列(由左到右表示從舊到新)\n",
    "    - 指標包含：Bias(乖離率), OV(成交量偏離), VWAP_STD(VWAP標準差)\n",
    "    \n",
    "    【盤勢判斷規則】\n",
    "    以下分類需根據Bias, OV, VWAP_STD 的數值變化判斷，而不是只看單一點\n",
    "    1. **弱勢震盪**\n",
    "       數值小幅度變動, Bias, OV, VWAP_STD數值幾乎沒變化。\n",
    "       舉例如下：\n",
    "        - Bias: [0.02,0.05,0.10,0.10,0.12,0.10,0.09,0.08,0.08,0.08,0.08,0.06,0.13,0.14,0.10,0.08,0.06,0.04,0.03,0.02,0.05]\n",
    "        - OV: [0.02,0.03,0.03,0.03,0.03,0.03,0.02,0.02,0.02,0.02,0.03,0.03,0.04,0.04,0.03,0.03,0.02,0.02,0.01,0.01,0.02]\n",
    "        - VWAP_STD: [2.53,2.52,2.52,2.52,2.51,2.50,2.50,2.50,2.50,2.49,2.48,2.48,2.47,2.46,2.46,2.45,2.45,2.45,2.45,2.44,2.43]\n",
    "    \n",
    "    2. **無量上漲 / 無量下跌**\n",
    "       Bias和OV數值會從大數值變小後, Bias沒什麼變化但OV持續變小, 但VWAP_STD數值有持續的再上升沒回落。\n",
    "       舉例如下：\n",
    "        - Bias: [0.92,0.49,0.31,0.26,0.36,0.47,0.37,0.34,0.29,0.23,0.18,0.45]\n",
    "        - OV: [0.23,0.18,0.12,0.11,0.11,0.11,0.08,0.07,0.07,0.05,0.05,0.08]\n",
    "        - VWAP_STD: [0.59,0.59,0.59,0.58,0.60,0.63,0.63,0.65,0.65,0.65,0.65,0.73]\n",
    "    \n",
    "    4. **多頭趨勢**\n",
    "       Bias和VWAP_STD數值持續上升, OV可能會有些許的回落但也會比相對初期的數值還大。\n",
    "       舉例如下：\n",
    "        - Bias: [0.13,0.14,0.35,2.77,2.86,2.76,4.47,5.45,8.97,8.87]\n",
    "        - OV: [0.07,0.07,0.09,0.31,0.28,0.23,0.27,0.24,0.28,0.25]\n",
    "        - VWAP_STD: [1.44,1.44,1.54,2.80,3.01,3.23,4.31,5.62,8.09,8.89]\n",
    "    \n",
    "    5. **多頭趨勢後期**\n",
    "       Bias數值出現最大後會開始縮小, OV也開始轉小, VWAP_STD可能還持續維持在高數值。\n",
    "       舉例如下：\n",
    "        - Bias: [2.76,4.47,5.45,8.97,8.87,6.09]\n",
    "        - OV: [0.23,0.27,0.24,0.28,0.25,0.17]\n",
    "        - VWAP_STD: [3.23,4.31,5.62,8.09,8.89,8.89]\n",
    "\n",
    "    10. **震盪整理**\n",
    "       當Bias, OV, VWAP_STD曾經經歷過數值大幅度上升後, 數值都開始變小。\n",
    "       舉例如下：\n",
    "        - Bias: [0.66,0.68,0.72,1.48,3.19,2.86,2.27,1.68,1.18,0.85]\n",
    "        - OV: [0.08,0.08,0.08,0.10,0.16,0.14,0.12,0.10,0.08,0.08]\n",
    "        - VWAP_STD: [1.00,1.06,1.15,1.62,2.43,2.57,2.59,2.57,2.55,2.51]\n",
    "    \n",
    "    【輸出格式】\n",
    "    使用上述規定的盤勢種類, 依照對每個商品的盤勢分析結果, 嚴格按照以下格式純文字輸出, 不要帶有json或其他格式（所有商品都要回覆）：\n",
    "        [商品分析]\n",
    "        商品xxx: 分析結果\n",
    "        商品xxx: 分析結果\n",
    "        商品xxx: 分析結果\n",
    "        ...\n",
    "\n",
    "        [總結]\n",
    "        簡述整體市場狀況，並把相同走勢的商品彙整，例如：\n",
    "        「商品xxx與商品yyy皆為震盪」\n",
    "\n",
    "    【重要要求】\n",
    "    1. 必須分析所有輸入的商品，不得遺漏。\n",
    "    2. 並且分析到最新時刻(最右邊)。\n",
    "    3. 必須綜合考慮 Bias, OV, VWAP_STD 三個指標。\n",
    "    4. 判斷需依照整段時間序列的整體結構，不可只根據單點。\n",
    "    5. 不需要對於指標數值的分析, 明確表示截至當前時刻商品屬於什麼盤勢。\n",
    "\"\"\").strip()\n",
    "\n",
    "system_prompt_step2 = textwrap.dedent(\"\"\"\n",
    "    你是一位頂尖的量化配對交易員，你的任務是從當前給予的時間序列資料和當前給予的盤勢變化, 以及給予整個族群的商品組, 進行分析並判斷誰相對領漲誰相對弱後。\n",
    "    \n",
    "    【限制】:\n",
    "    1. **僅使用輸入數據**：\n",
    "        - 只能使用輸入數據中提供的資料如K, Volume等。\n",
    "        - **禁止引入任何非輸入數據中提供的指標或策略方法**。\n",
    "        - 分析語言必須完全基於數據，不得包含策略、理論或技術分析方法描述。\n",
    "    2. **你的所有分析都必須明確引用輸入數據中的「時間範圍」，將你的發現錨定在具體的時間上下文中, 並且在執行分析的時候, 必須使用分析期時間範圍作為時間軸, 審視時間上的輸入數據的變化。**\n",
    "    3. **每組商品的資料都是由左至右表示時間由舊到新\n",
    "    \n",
    "    【數據結構定義】:\n",
    "    數據格式如下:\n",
    "    **分析期 (time_window)**: 時間範圍 (time_window), 當前所有數據的時間序列都基於此時間段, 你的所有分析都基於這個時期的數據變化。\n",
    "    **舉例**:\n",
    "        分析時間段為: 2024-01-31 00:00:00 to 2024-02-05 00:00:00\n",
    "        - K (Close): [7.86, 7.83, 7.8, 7.81, 7.82, 7.78]\n",
    "        - Volume: [619.0,704.0,779.0,499.0,539.0,687]\n",
    "        \n",
    "        則代表K的時間序列為從01/31至02/05為\n",
    "                K       Volume\n",
    "        01/31   7.86    619\n",
    "        02/01   7.83    704\n",
    "        02/02   7.8     779\n",
    "        02/03   7.81    499\n",
    "        02/04   7.82    539\n",
    "        02/05   7.78    687\n",
    "    **分析期時間範圍就會對應到時間序列陣列中逗號隔開的每個數值\n",
    "    \n",
    "    【任務】：\n",
    "    **分析當前盤勢以及判斷當前的所有商品組內, 哪個商品有相對領漲的情況, 並由高到低排序出當前輸入的商品組的走勢誰高誰低**\n",
    "    **尤其特別關注Volume的變化, 他是整體動能的關鍵**\n",
    "    \n",
    "    【請遵守以下規則】：\n",
    "    **能量排序**\n",
    "        **任務**: 先對當前的時間範圍內依照盤勢結果, 判斷當前是要使用哪種類型的配對交易, 再接著進行價格能量判斷, 誰領漲或誰領跌, 給予出一個商品之間的能量排序, 流程如下。\n",
    "        a. **交易類型**:\n",
    "            **觀察期**:\n",
    "            - 使用時機: 當分析的盤勢是「多頭假趨勢，空頭假趨勢, 多頭趨勢初期, 空頭趨勢初期」, 截至目前還無法判定後續盤勢。\n",
    "            - 結束時機: 當分析的盤勢是「多頭趨勢中期，空頭趨勢中期, 無量盤整價跌，無量盤整價增, 無量盤整」, 已確定當前的盤勢情形，依照後續結果決定當前行為。\n",
    "        \n",
    "            **動量反轉收斂型配對交易**:\n",
    "            - 使用時機: 當分析的盤勢是「多頭趨勢後期，空頭趨勢後期」, 預計後續趨勢會進入後期。\n",
    "            - 結束時機: 當分析的盤勢是「無量盤整價增，無量盤整價跌, 無量盤整」, 預計後續量縮將會盤整, 將在後續持續觀察, 並依照盤勢情況決定是否替換為「均值回歸發散型配對交易」。\n",
    "        \n",
    "            **均值回歸發散型配對交易**:\n",
    "            - 使用時機: 當分析的盤勢是「無量盤整價跌，無量盤整價增, 無量盤整」, 預期價格會在一個區間內來回穿梭。\n",
    "            - 結束時機: 當分析的盤勢是「多頭假趨勢，空頭假趨勢, 多頭趨勢初期, 空頭趨勢初期」, 預計後續可能會出量變趨勢, 將在後續持續觀察, 並依照盤勢情況決定是否替換為「動量反轉收斂型配對交易」。\n",
    "            \n",
    "        b. **商品組相對強弱排序（量價效率法，適用多頭、空頭、盤整）**\n",
    "           核心原則：誰「每1張成交量換到最多價格變化」（漲最多或抗跌最多）＝當前最強。\n",
    "           必須嚴格一步一步照以下格式計算，未來所有判斷都必須100%模仿下面4個完整範例。\n",
    "           Step 1（決勝負，權重90%）：量價效率 = |期間漲跌幅%| ÷ 期間總成交量（保留8位小數）\n",
    "           Step 2（僅差距 < 15%時看）：總漲跌幅%\n",
    "           Step 3（僅差距 < 8%時看）：最近3根K的量價表現 + Bias最後值 + OV最後值\n",
    "           最終排序永遠只輸出：當前商品量能排序為: [最強, 次強, 最弱]\n",
    "           \n",
    "           【範例1：多頭暴漲期】  \n",
    "           （略）資料同你原本舉例1  \n",
    "           Step 1：A 0.00018370　B 0.00008420　C 0.00028610 → C領先A 55.7%  \n",
    "           最終：當前商品量能排序為: [C, A, B]\n",
    "           \n",
    "           【範例2：無量震盪盤整期】  \n",
    "           （略）資料同你原本舉例2  \n",
    "           Step 1：A 0.00001469　B 0.00019876　C 0.00006995 → B領先C 184.2%  \n",
    "           最終：當前商品量能排序為: [B, C, A]\n",
    "           \n",
    "           【範例3：空頭暴跌期（真正抗跌者為最強）】\n",
    "           商品A: K [58.0 → 42.5] 期間-26.72%　總量 1,280,000 → 效率 0.00020869  \n",
    "           商品B: K [32.0 → 28.5] 期間-10.94%　總量 185,000   → 效率 0.00005914  \n",
    "           商品C: K [95.0 → 68.0] 期間-28.42%　總量 980,000   → 效率 0.00028982  \n",
    "           Step 1：C (0.00028982) > A (0.00020869) > B (0.00005914) （C最抗跌）  \n",
    "           最終：當前商品量能排序為: [C, A, B]\n",
    "           \n",
    "           【範例4：空頭末段無量續跌（最不跌者為最強）】\n",
    "           商品A: K [45.0 → 43.8] 期間-2.67%　總量 42,000 → 效率 0.00006357  \n",
    "           商品B: K [28.0 → 24.5] 期間-12.50%　總量 68,000 → 效率 0.00018382  \n",
    "           商品C: K [72.0 → 71.0] 期間-1.39%　總量 18,000 → 效率 0.00007722  \n",
    "           Step 1：B用最多量才跌最多＝最弱；A與C差距小但A量更大 → A略強  \n",
    "           最終：當前商品量能排序為: [A, C, B]\n",
    "           \n",
    "           重要提醒：\n",
    "           - 多頭看誰漲最多最省力 → 最強\n",
    "           - 空頭看誰跌最少最省力（或用同樣量跌最少）→ 最強\n",
    "           - 盤整期同上，永遠只看「每1張換到多少價格位移」\n",
    "           - 只要Step 1差距 ≥ 15%，直接按Step 1排序，Step 2/3可省略\n",
    "    \n",
    "    【輸出格式】：\n",
    "    請針對【規則】的商品排序結果以及輸入的盤勢情形進行輸出, 格式如下:\n",
    "    1. 當前時間段: (time_window)。\n",
    "    2. 交易型態: 依照盤勢過程, 來輸出判斷過往的交易類型, 以及對最新時刻的盤勢判斷是否需要替換交易類型。\n",
    "        - (0)觀察期不操作\n",
    "        - (1)動量反轉收斂型配對交易\n",
    "        - (2)均值回歸發散型配對交易\n",
    "        輸出舉例: 0 -> 2 -> 0 -> 1\n",
    "    \n",
    "    3.能量排序: 依照【規則】的能量排序進行輸出。\n",
    "        - 輸出舉例: [商品A, 商品B, ...等]\n",
    "        \n",
    "    請根據以上框架，對過往的數據進行盤勢分析和量能排序，確保所有結論都包含明確的時間參考, 依照輸出格式進行輸出。\n",
    "    \"\"\").strip()\n",
    "\n",
    "def clean_think_content(text: str) -> str:\n",
    "    # 將 <think> 到 </think> 中間的內容整段移除\n",
    "    cleaned_text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "    # 去掉前後多餘空白\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def market_trend(input_text):\n",
    "    # 建立 ChatPromptTemplate\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt_step1),\n",
    "        (\"human\", \"{input_data}\")  # 使用變量占位符\n",
    "    ])\n",
    "    chain = chat_prompt | llm\n",
    "    response = chain.invoke({\"input_data\": input_text})\n",
    "    summary = clean_think_content(response.content.strip())\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def relative_strength(input_text, trend):\n",
    "    # 建立 ChatPromptTemplate\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt_step2),\n",
    "        (\"human\", \"{input_data}\")  # 使用變量占位符\n",
    "    ])\n",
    "    chain = chat_prompt | llm\n",
    "    response = chain.invoke({\"input_data\": input_text})\n",
    "    summary = clean_think_content(response.content.strip())\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# 將 LLM 分析的 for loop 封裝成一個 function\n",
    "def process_windows(df_all, CODE, json_file, k_lookback=10, N=5):\n",
    "    if os.path.exists(json_file): # 如果 JSON 已存在，刪除它以重新開始\n",
    "        print(f\"找到舊檔 {json_file}，將其刪除並重新開始分析。\")\n",
    "        os.remove(json_file)\n",
    "\n",
    "    # 無論如何都從一個空的 list 開始\n",
    "    summaries = []\n",
    "    \n",
    "    for i in range(0, len(df_all) - k_lookback + 1):\n",
    "        df = df_all.iloc[i:i + k_lookback].copy()\n",
    "        product_info_list = []\n",
    "        \n",
    "        for code in CODE:\n",
    "            target_cols = [f'vwap_std_{code}', f'{code}_bias', f'{code}_ov']\n",
    "            df_subset = df[target_cols].copy()\n",
    "\n",
    "            # 取得時間 (只需做一次，或在此提取)\n",
    "            start_time = df_subset.index[0].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            end_time = df_subset.index[-1].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            time_window = f\"{start_time} to {end_time}\"\n",
    "\n",
    "            # 數據處理：四捨五入 -> 轉列表 -> 轉字串 -> 去除空格 (Token 最佳化)\n",
    "            vwap_std_str = str(df_subset[f'vwap_std_{code}'].round(2).tolist()).replace(\" \", \"\")\n",
    "            bias_str = str(df_subset[f'{code}_bias'].round(2).tolist()).replace(\" \", \"\")\n",
    "            ov_str = str(df_subset[f'{code}_ov'].round(2).tolist()).replace(\" \", \"\")\n",
    "\n",
    "            # --- 關鍵修改：使用明確的 \\n 串接，不使用多行字串符號, 這樣可以保證 \"商品\" 絕對靠左，而數據絕對縮排 4 格\n",
    "            product_block = (\n",
    "                f\"- 商品 {code}:\\n\"\n",
    "                f\"    - VWAP_STD: {vwap_std_str}\\n\"\n",
    "                f\"    - Bias: {bias_str}\\n\"\n",
    "                f\"    - OV: {ov_str}\"\n",
    "            )\n",
    "\n",
    "            product_info_list.append(product_block)\n",
    "\n",
    "        # 呼叫 LLM\n",
    "        dynamic_products_content = \"\\n\\n\".join(product_info_list) # 將所有商品區塊合併，中間空一行\n",
    "        print(f\"正在分析窗口：{time_window}...\")\n",
    "        try:\n",
    "            input_prompt = textwrap.dedent(f\"\"\"\n",
    "                請分析以下時間窗口的數據：\n",
    "                **分析期**： {start_time} to {end_time}\n",
    "\n",
    "                **商品組**:\n",
    "                {dynamic_products_content}\n",
    "\n",
    "                請根據以上數據，使用「分析時間段分析出合適的資料」並依照規定進行輸出。\n",
    "            \"\"\").strip()\n",
    "\n",
    "            trend_raw = market_trend(input_prompt)\n",
    "        except Exception as e:\n",
    "            print(f\"Trend tool error: {e}\")\n",
    "        \n",
    "        # 儲存成 dict\n",
    "        summaries.append({\n",
    "            \"time_window\": time_window,\n",
    "            \"trend_analysis\": clean_think_content(trend_raw),\n",
    "            # \"rs_analysis\": rs_summary\n",
    "        })\n",
    "\n",
    "        # 立即 append 到 JSON (避免中斷遺失)\n",
    "        with open(json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(summaries, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "            \n",
    "        \n",
    "        # break\n",
    "        \n",
    "\n",
    "        print(f\"完成窗口：{time_window}\")\n",
    "\n",
    "    print(\"\\n預處理完成，所有總結存於\", json_file)\n",
    "    return summaries\n",
    "\n",
    "summaries = process_windows(df_final, CODE, JSON_FILE, k_lookback=8, N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacfb6c4-0611-4453-8ecc-0c67fec584b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
